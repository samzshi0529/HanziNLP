# HanziNLP

ä¸€ä¸ª**ç”¨æˆ·å‹å¥½**ä¸”**æ˜“äºä½¿ç”¨**çš„è‡ªç„¶è¯­è¨€å¤„ç†åŒ…ï¼Œä¸“ä¸ºä¸­æ–‡æ–‡æœ¬åˆ†æã€å»ºæ¨¡å’Œå¯è§†åŒ–è€Œè®¾è®¡ã€‚HanziNLPä¸­çš„æ‰€æœ‰åŠŸèƒ½éƒ½æ”¯æŒä¸­æ–‡æ–‡æœ¬ï¼Œå¹¶ä¸”éå¸¸é€‚ç”¨äºä¸­æ–‡æ–‡æœ¬åˆ†æï¼

å¦‚æœHanziNLPèƒ½å¤Ÿå¸®åˆ°ä½ ï¼Œéå¸¸å¸Œæœ›èƒ½å¤Ÿè¯·ä½ ç»™æœ¬repositoryç‚¹ä¸ŠğŸŒŸï¼

<details>
<summary>ğŸ‡¨ğŸ‡³ Chinese Version (ç‚¹å‡»æŸ¥çœ‹ä¸­æ–‡ç‰ˆæœ¬,ç”±GPT-4ç¿»è¯‘å®Œæˆ)</summary>
  
## ç›®å½•
- [1. å¿«é€Ÿå¼€å§‹](#1-å¿«é€Ÿå¼€å§‹)
  - [1.1 ç›¸å…³é“¾æ¥](#11-ç›¸å…³é“¾æ¥)
  - [1.2 å®‰è£…ä¸ä½¿ç”¨](#12-å®‰è£…ä¸ä½¿ç”¨)
  - [1.3 äº¤äº’å¼ä»ªè¡¨æ¿](#13-äº¤äº’å¼ä»ªè¡¨æ¿)
- [2. å­—ç¬¦å’Œè¯æ±‡è®¡æ•°](#2-å­—ç¬¦å’Œè¯æ±‡è®¡æ•°)
- [3. å­—ä½“ç®¡ç†](#3-å­—ä½“ç®¡ç†)
- [4. æ–‡æœ¬åˆ†æ®µ](#4-æ–‡æœ¬åˆ†æ®µ)
  - [4.1 åœç”¨è¯ç®¡ç†](#41-åœç”¨è¯ç®¡ç†)
  - [4.2 å¥å­åˆ†æ®µ](#42-å¥å­åˆ†æ®µ)
  - [4.3 è¯è¯­æ ‡è®°](#43-è¯è¯­æ ‡è®°)
- [5. æ–‡æœ¬è¡¨ç¤º](#5-æ–‡æœ¬è¡¨ç¤º)
  - [5.1 è¯è¢‹æ¨¡å‹ (BoW)](#51-è¯è¢‹æ¨¡å‹-bow)
  - [5.2 ngrams](#52-ngrams)
  - [5.3 TF_IDF (è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡)](#53-tf_idf-è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡)
  - [5.4 TT_matrix (è¯-è¯çŸ©é˜µ)](#54-tt_matrix-è¯-è¯çŸ©é˜µ)
- [6. æ–‡æœ¬ç›¸ä¼¼æ€§](#6-æ–‡æœ¬ç›¸ä¼¼æ€§)
- [7. è¯åµŒå…¥](#7-è¯åµŒå…¥)
  - [7.1 Word2Vec](#71-word2vec)
  - [7.2 BERT åµŒå…¥](#72-bert-åµŒå…¥)
- [8. ä¸»é¢˜å»ºæ¨¡](#8-ä¸»é¢˜å»ºæ¨¡)
  - [8.1 æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é… (LDA) æ¨¡å‹](#81-æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…-lda-æ¨¡å‹)
  - [8.2 LDA print_topics å‡½æ•°](#82-lda-print_topics-å‡½æ•°)
- [9. æƒ…æ„Ÿåˆ†æ](#9-æƒ…æ„Ÿåˆ†æ)
- [åœ¨æ‚¨çš„ç ”ç©¶ä¸­å¼•ç”¨HanziNLP](#åœ¨æ‚¨çš„ç ”ç©¶ä¸­å¼•ç”¨HanziNLP)

## å¼€å‘è€…å¤‡æ³¨ï¼š

å¯¹äºä»»ä½•ä½¿ç”¨HanziNLPçš„äºº,å¼€å‘è€…å‘æ‚¨è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼ğŸ‰ğŸ‰ğŸ‰

å…³äºæˆ‘æ›´å¤šçš„ä¿¡æ¯å’Œä»»ä½•æ”¹è¿›çš„å»ºè®®ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ‰¾åˆ°æˆ‘ï¼š
- **ä¸ªäººé‚®ç®±**ï¼šsamzshi@sina.com
- **ä¸ªäººç½‘ç«™**ï¼š[https://www.samzshi.com/](https://www.samzshi.com/)
- **é¢†è‹±**ï¼š[www.linkedin.com/in/zhanshisamuel](www.linkedin.com/in/zhanshisamuel)

## 1. å¿«é€Ÿå¼€å§‹

æ¬¢è¿æ¥åˆ° **HanziNLP** ğŸŒŸ - ä¸€ä¸ªå³ç”¨çš„ä¸­æ–‡æ–‡æœ¬è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å·¥å…·åŒ…ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒè‹±æ–‡ã€‚å®ƒæ—¨åœ¨æˆä¸ºå³ä¾¿æ˜¯Pythonæ–°æ‰‹ä¹Ÿèƒ½å‹å¥½ä½¿ç”¨çš„ç®€åŒ–å·¥å…·ã€‚

æ­¤å¤–ï¼ŒHanziNLPè¿˜æä¾›äº†ä¸€ä¸ªäº¤äº’å¼ä»ªè¡¨æ¿ï¼Œç”¨äºåŠ¨æ€æ´å¯ŸNLPåŠŸèƒ½ï¼Œä¸ºå„ç§NLPåŠŸèƒ½æä¾›åŠ¨æ€æ¦‚è§ˆå’Œæ´å¯Ÿã€‚

### 1.1 ç›¸å…³é“¾æ¥

- **GitHubä»“åº“**ï¼šåœ¨[GitHub](https://github.com/samzshi0529/HanziNLP)ä¸Šæ¢ç´¢æˆ‘çš„ä»£ç å¹¶åšå‡ºè´¡çŒ®ã€‚
- **PyPIé¡µé¢**ï¼šåœ¨[PyPI](https://libraries.io/pypi/HanziNLP)ä¸Šæ‰¾åˆ°æˆ‘ï¼Œå¹¶æ¢ç´¢æ›´å¤šå…³äºå¦‚ä½•å°†HanziNLPé›†æˆåˆ°æ‚¨çš„é¡¹ç›®ä¸­çš„ä¿¡æ¯ã€‚

### 1.2 å®‰è£…ä¸ä½¿ç”¨

ä½¿ç”¨HanziNLPåªéœ€æ‰§è¡Œä¸€ä¸ªç®€å•çš„å‘½ä»¤å³å¯å¼€å§‹ï¼

```python
pip install HanziNLP
```

### 1.3 äº¤äº’å¼ä»ªè¡¨æ¿

![æ›¿ä»£æ–‡æœ¬](README_PIC/dashboard_video.gif)

#### é€šè¿‡ä¸€è¡Œç®€å•çš„ä»£ç ä½¿ç”¨ dashboard()ï¼

```python
from HanziNLP import dashboard
dashboard()
```

- **å‡½æ•°**ï¼š`dashboard()`
- **ç›®çš„**ï¼šå±•ç¤ºä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ä»ªè¡¨æ¿ï¼Œä¾¿äºè¿›è¡Œäº¤äº’å¼æ–‡æœ¬åˆ†æå’Œæƒ…æ„Ÿåˆ†ç±»ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè§‚å¯Ÿå„ç§é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å‚æ•°å¯¹å¤„ç†æ–‡æœ¬çš„å½±å“ï¼Œä»è€Œé€‰æ‹©æœ€é€‚åˆä»–ä»¬ç”¨ä¾‹çš„æ¨¡å‹å’Œå‚æ•°ã€‚
- **å‚æ•°**ï¼šä¸éœ€è¦å‚æ•°ã€‚
- **è¿”å›**ï¼šæ— è¿”å›å€¼ï¼›è¯¥å‡½æ•°è¾“å‡ºä¸€ä¸ªä»ªè¡¨æ¿ç•Œé¢ã€‚

#### æ¦‚è¿°

`dashboard` å‡½æ•°å¼•å…¥äº†ä¸€ä¸ªç”¨æˆ·äº¤äº’å¼çš„ä»ªè¡¨æ¿ï¼Œæ—¨åœ¨æ‰§è¡Œæ–‡æœ¬åˆ†æå’Œæƒ…æ„Ÿåˆ†ç±»ï¼Œä¸ºç”¨æˆ·æä¾›äº²èº«ä½“éªŒï¼Œä»¥æ¢ç´¢å’Œç†è§£ä¸åŒé¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å‚æ•°å¯¹æ–‡æœ¬å¤„ç†çš„å½±å“ã€‚

- **äº¤äº’å¼æ–‡æœ¬åˆ†æ**ï¼šç”¨æˆ·å¯ä»¥è¾“å…¥æ–‡æœ¬ï¼Œè§‚å¯Ÿå„ç§æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯ï¼Œä¾‹å¦‚å•è¯è®¡æ•°ã€å­—ç¬¦è®¡æ•°å’Œå¥å­è®¡æ•°ï¼Œå¹¶å¯è§†åŒ–è¯é¢‘å’Œæƒ…æ„Ÿåˆ†ç±»ç»“æœã€‚
- **æ¨¡å‹æ¢ç´¢**ï¼šç”¨æˆ·å¯ä»¥é€‰æ‹©ä» Hugging Face æŒ‡å®šä¸€ä¸ªåˆ†ç±»æ¨¡å‹ã€‚å¦‚æœç•™ç©ºï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹ 'uer/roberta-base-finetuned-chinanews-chinese'ã€‚æœ‰å…³æ­¤æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—® [Hugging Face](https://huggingface.co/uer/roberta-base-finetuned-chinanews-chinese)ã€‚
- **åˆ†è¯å‚æ•°è°ƒæ•´**ï¼šç”¨æˆ·å¯ä»¥è°ƒæ•´åˆ†è¯è®¾ç½®ï¼Œä¾‹å¦‚ 'Jieba Mode' å‚æ•°å’Œåœç”¨è¯é€‰æ‹©ï¼Œå¹¶è§‚å¯Ÿç”Ÿæˆçš„è¯è¯­åŠå…¶å„è‡ªçš„é¢‘ç‡ã€‚
- **å¯è§†åŒ–**ï¼šä»ªè¡¨æ¿æä¾›äº†æ–‡æœ¬ç»Ÿè®¡ã€è¯é¢‘å’Œæƒ…æ„Ÿåˆ†ç±»çš„è§†è§‰æ´å¯Ÿï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£æ–‡æœ¬åˆ†æç»“æœã€‚
- **æƒ…æ„Ÿåˆ†ç±»**ï¼šä»ªè¡¨æ¿ä½¿ç”¨æŒ‡å®šçš„ï¼ˆæˆ–é»˜è®¤çš„ï¼‰æ¨¡å‹æ‰§è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼Œå¹¶æ˜¾ç¤ºæƒ…æ„Ÿæ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒã€‚

#### äº®ç‚¹

`dashboard` å‡½æ•°å¼ºè°ƒ**ç”¨æˆ·å‚ä¸**å’Œ**æ¢ç´¢**ã€‚å®ƒå…è®¸ç”¨æˆ·ä¸å„ç§é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å‚æ•°è¿›è¡Œäº¤äº’å¼äº¤æµï¼Œè§‚å¯Ÿå®ƒä»¬å¯¹æ–‡æœ¬åˆ†æå’Œæƒ…æ„Ÿåˆ†ç±»çš„å½±å“ã€‚è¿™ç§äº¤äº’å¼æ¢ç´¢ä½¿ç”¨æˆ·èƒ½å¤Ÿåšå‡ºæ˜æ™ºçš„å†³ç­–ï¼Œé€‰æ‹©æœ€ç¬¦åˆä»–ä»¬ç‰¹å®šç”¨ä¾‹çš„æ¨¡å‹å’Œå‚æ•°ï¼Œä»è€Œå¢å¼ºä»–ä»¬çš„æ–‡æœ¬åˆ†æå’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ã€‚

## 2. å­—ç¬¦å’Œè¯æ±‡è®¡æ•°

ğŸš€ è¿™ä¸ªåŸºæœ¬åŠŸèƒ½è®¡ç®—æ–‡æœ¬ä¸­çš„å­—ç¬¦å’Œå•è¯æ•°é‡ï¼Œçœå»äº†æ‚¨è‡ªå·±è¯†åˆ«å’Œåˆ†å‰²ä¸­æ–‡å•è¯çš„æ‰‹åŠ¨åŠªåŠ›ã€‚

### char_freq å’Œ word_freq å‡½æ•°
- `char_freq(text, text_only=True)`: å‡½æ•°ç”¨äºè®¡ç®—ç»™å®šæ–‡æœ¬ä¸­æ¯ä¸ªå­—ç¬¦çš„é¢‘ç‡ï¼›å¦‚æœ text_only == Trueï¼Œåªä¼šè®¡ç®—ä¸­æ–‡å’Œè‹±æ–‡å­—ç¬¦ã€‚å¦‚æœ text_only == Falseï¼Œå°†è®¡ç®—æ‰€æœ‰å­—ç¬¦ã€‚é»˜è®¤ä¸º Trueã€‚
- `word_freq(text)`: å‡½æ•°ç”¨äºè®¡ç®—ç»™å®šæ–‡æœ¬ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡ã€‚
### ç¤ºä¾‹
```python
from HanziNLP import char_freq, word_freq

text = "ä½ å¥½, ä¸–ç•Œ!"
char_count = char_freq(text)
word_count = word_freq(text)

print(f"å­—ç¬¦è®¡æ•°: {char_count}")
print(f"å•è¯è®¡æ•°: {word_count}")
```
### è¾“å‡º 
```python
å­—ç¬¦è®¡æ•°: 4
å•è¯è®¡æ•°: 2
```
## 3. å­—ä½“ç®¡ç†

åœ¨Pythonç¯å¢ƒä¸­å¯è§†åŒ–ä¸­æ–‡æ–‡æœ¬æ—¶ï¼Œå­—ä½“æ˜¯ä¸€ä¸ªç»å¸¸éœ€è¦æ‰‹åŠ¨å¯¼å…¥çš„é‡è¦èµ„æºã€‚HanziNLPå†…ç½®äº†å­—ä½“åˆ—è¡¨ï¼Œå¯ç«‹å³ä½¿ç”¨ã€‚æ‚¨å¯ä»¥ä½¿ç”¨`list_fonts()`æŸ¥çœ‹å’Œè¿‡æ»¤æ‰€æœ‰å¯ç”¨çš„å­—ä½“ï¼Œå¹¶ä½¿ç”¨`get_font()`æ£€ç´¢ç”¨äºå¯è§†åŒ–ç›®çš„çš„ç‰¹å®šå­—ä½“è·¯å¾„ã€‚æ‰€æœ‰å†…ç½®çš„å­—ä½“éƒ½æ¥è‡ªGoogleå­—ä½“ï¼Œå®ƒä»¬æ ¹æ®å¼€æ”¾å­—ä½“è®¸å¯è¯è·å¾—è®¸å¯ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥åœ¨äº§å“å’Œé¡¹ç›®ä¸­ä½¿ç”¨å®ƒä»¬â€”â€”æ— è®ºæ˜¯å°åˆ·å“è¿˜æ˜¯æ•°å­—å“ï¼Œæ— è®ºæ˜¯å•†ä¸šçš„è¿˜æ˜¯å…¶ä»–çš„ã€‚

### list_fonts å’Œ get_font å‡½æ•°
- `list_fonts()`: åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å­—ä½“ã€‚
- `get_font(font_name, show=True)`: æ£€ç´¢ç”¨äºå¯è§†åŒ–ç›®çš„çš„ç‰¹å®šå­—ä½“ã€‚å¦‚æœ show == Trueï¼Œå°†æ˜¾ç¤ºå­—ä½“çš„æ ·æœ¬å¯è§†åŒ–ã€‚å¦‚æœ show == Falseï¼Œå°†ä¸æ˜¾ç¤ºä»»ä½•å†…å®¹ã€‚é»˜è®¤è®¾ç½®ä¸ºTrueã€‚

#### list_fonts() ç¤ºä¾‹
```python
from HanziNLP import list_fonts

# åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å­—ä½“
list_fonts()
```
#### è¾“å‡º
![ç¤ºä¾‹å›¾ç‰‡](README_PIC/list_fonts().png)

#### get_font() ç¤ºä¾‹
```python
from HanziNLP import get_font

font_path = get_font('ZCOOLXiaoWei-Regular') #åœ¨ list_fonts() ä¸­è¾“å…¥æ‚¨å–œæ¬¢çš„ font_name
```
#### è¾“å‡º
![ç¤ºä¾‹å›¾ç‰‡](README_PIC/get_font.png)

#### è¯äº‘ç¤ºä¾‹
æ‚¨å¯ä»¥ä½¿ç”¨å®šä¹‰çš„ä¸­æ–‡ font_path åˆ¶ä½œå„ç§å›¾è¡¨ã€‚ä¸‹é¢æä¾›äº†ä¸€ä¸ªè¯äº‘ç¤ºä¾‹ï¼š
```python
from PIL import Image
from wordcloud import WordCloud,ImageColorGenerator
import matplotlib.pyplot as plt

# ç”±GPT-4ç”Ÿæˆçš„æ ·æœ¬æ–‡æœ¬
text = 'åœ¨æ˜åªšçš„æ˜¥å¤©é‡Œï¼Œå°èŠ±çŒ«å’ªæ‚ é—²åœ°èººåœ¨çª—å°ä¸Šï¼Œäº«å—ç€æ¸©æš–çš„é˜³å…‰ã€‚å¥¹çš„çœ¼ç›é—ªçƒç€å¥½å¥‡çš„å…‰èŠ’ï¼Œæ—¶ä¸æ—¶åœ°è§‚å¯Ÿç€çª—å¤–å¿™ç¢Œçš„å°é¸Ÿå’Œè´è¶ã€‚å°çŒ«çš„å°¾å·´è½»è½»æ‘‡åŠ¨ï¼Œè¡¨è¾¾ç€å¥¹å†…å¿ƒçš„èˆ’é€‚å’Œæ»¡è¶³ã€‚åœ¨å¥¹çš„èº«è¾¹ï¼Œä¸€ç›†ç››å¼€çš„ç´«ç½—å…°æ•£å‘ç€æ·¡æ·¡çš„é¦™æ°”ï¼Œç»™è¿™ä¸ªå®é™çš„åˆåå¢æ·»äº†å‡ åˆ†è¯—æ„ã€‚å°èŠ±çŒ«å’ªå¶å°”ä¼šé—­ä¸Šå¥¹çš„çœ¼ç›ï¼Œæ²‰æµ¸åœ¨è¿™ç¾å¥½çš„æ—¶å…‰ä¸­ï¼Œä»¿ä½›æ•´ä¸ªä¸–ç•Œéƒ½å˜å¾—æ¸©é¦¨å’Œè°ã€‚çª—å¤–çš„æ¨±èŠ±æ ‘åœ¨å¾®é£ä¸­è½»è½»æ‘‡æ›³ï¼Œæ´’ä¸‹ä¸€ç‰‡ç‰‡ç²‰è‰²çš„èŠ±ç“£ï¼Œå¦‚æ¢¦å¦‚å¹»ã€‚åœ¨è¿™æ ·çš„ä¸€ä¸ªæ‚ æ‰˜çš„æ˜¥æ—¥é‡Œï¼Œä¸€åˆ‡éƒ½æ˜¾å¾—å¦‚æ­¤ç¾å¥½å’Œå¹³é™ã€‚'

text = " ".join(text)

# ç”Ÿæˆè¯äº‘
wordcloud = WordCloud(font_path= font_path, width=800, height=800,
                      background_color='white',
                      min_font_size=10).generate(text)

# æ˜¾ç¤ºè¯äº‘
plt.figure(figsize=(5, 5), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.title("æ ·æœ¬è¯äº‘")

plt.show()
```
#### è¾“å‡º
![ç¤ºä¾‹å›¾ç‰‡](README_PIC/wordcloud.png)

## 4. æ–‡æœ¬åˆ†æ®µ
æ–‡æœ¬åˆ†æ®µæ˜¯ä»»ä½•NLPä»»åŠ¡ä¸­çš„ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚ä¸€èˆ¬çš„æ­¥éª¤æ˜¯åˆ†æ®µå¥å­ï¼Œå»é™¤åœç”¨è¯ï¼Œå¹¶åˆ†åˆ«å¯¹æ¯ä¸ªå¥å­è¿›è¡Œåˆ†è¯ã€‚ä¸‹é¢ä»‹ç»äº†è¯¦ç»†çš„è¯´æ˜ã€‚

### 4.1 åœç”¨è¯ç®¡ç†
ä¸ºäº†åœ¨ä¸­æ–‡æ–‡æœ¬ä¸­å»é™¤åœç”¨è¯ï¼Œè¯¥åŒ…å†…ç½®äº†å¸¸è§çš„åœç”¨è¯åˆ—è¡¨ï¼ŒåŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªï¼šï¼ˆéƒ¨åˆ†åœç”¨è¯æ¥è‡ª[stopwords](https://github.com/goto456/stopwords/)ï¼‰

| åœç”¨è¯åˆ—è¡¨ | æ–‡ä»¶å |
|----------|----------|
| ä¸­æ–‡åœç”¨è¯è¡¨ | cn_stopwords.txt |
| å“ˆå·¥å¤§åœç”¨è¯è¡¨ | hit_stopwords.txt |
| ç™¾åº¦åœç”¨è¯è¡¨ | baidu_stopwords.txt |
| å››å·å¤§å­¦æœºå™¨æ™ºèƒ½å®éªŒå®¤åœç”¨è¯è¡¨ | scu_stopwords.txt |
| å¸¸ç”¨åœç”¨è¯è¡¨ | common_stopwords.txt |

#### list_stopwords å’Œ load_stopwords å‡½æ•°
- `list_stopwords()`: åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åœç”¨è¯ã€‚
- `load_stopwords(file_name)`: ä»æŒ‡å®šçš„æ–‡ä»¶åŠ è½½åœç”¨è¯åˆ°ä¸€ä¸ªè¯åˆ—è¡¨ã€‚ç„¶åï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹å¹¶åœ¨åç»­ä½¿ç”¨ä¸­ä½¿ç”¨è¿™äº›åœç”¨è¯ã€‚

##### list_stopwords ç¤ºä¾‹
```python
from HanziNLP import list_stopwords

list_stopwords()
```
##### è¾“å‡º 
![ç¤ºä¾‹å›¾ç‰‡](README_PIC/list_stopwords.png)

##### load_stopwords ç¤ºä¾‹
```python
from HanziNLP import load_stopwords

stopwords = load_stopwords('common_stopwords.txt') # åœ¨è¿™é‡Œè¾“å…¥txtæ–‡ä»¶å
```
##### è¾“å‡º 
```python
{'ç„¶è€Œ',
 'whoever',
 'åªé™',
 'çš„ç¡®',
 'è¦ä¸ç„¶',
 'each',
 'ä»æ—§',
 'è¿™ä¹ˆç‚¹å„¿',
 'å†’',
 'å¦‚æœ',
 'æ¯”åŠ',
 'ä»¥æœŸ',
 'çŠ¹è‡ª'.....
}
```
### 4.2 å¥å­åˆ†æ®µ
æ­¤åŠŸèƒ½å°†æ•´ä¸ªæ–‡æ¡£æˆ–æ®µè½åˆ†æ®µæˆå¥å­ã€‚æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æ–‡æœ¬ã€‚
- `sentence_segment(text)`: å°†è¾“å…¥æ–‡æœ¬åˆ†æ®µæˆå¥å­ã€‚

#### sentence_segment ç¤ºä¾‹ï¼šæ­¤ç¤ºä¾‹æ•…æ„é€‰æ‹©ä¸€ä¸ªéš¾ä»¥åˆ†å‰²çš„å¥å­ã€‚
```python
from HanziNLP import sentence_segment

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
sentence_segment(sample_sentence)
```
#### è¾“å‡º 
```python
['hello world!', 'This is Sam.', 'ã€‚', 'é™¤éä½ ä¸è¯´ã€‚', 'æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .', 'youã€‚']
```

### 4.3 è¯è¯­æ ‡è®°
ä½œä¸ºé¢„å¤„ç†æ–‡æœ¬ç”¨äºNLPä»»åŠ¡çš„æœ€é‡è¦æ­¥éª¤ä¹‹ä¸€ï¼Œ`word_tokenize()` å‡½æ•°æä¾›äº†ä¸€ç§ç›´æ¥å°†åŸå§‹ä¸­æ–‡æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°çš„æ–¹æ³•ã€‚

- **å‡½æ•°**ï¼š`word_tokenize(text, mode='precise', stopwords='common_stopwords.txt', text_only=False, include_numbers=True, custom_stopwords=None, exclude_default_stopwords=False)`
- **ç›®çš„**ï¼šå°†è¾“å…¥æ–‡æœ¬æ ‡è®°ä¸ºè¯ï¼ŒåŒæ—¶æä¾›æœ‰æ•ˆç®¡ç†åœç”¨è¯çš„é€‰é¡¹ã€‚
  
#### å‚æ•°ï¼š
- `text` (str): è¾“å…¥çš„ä¸­æ–‡æ–‡æœ¬ã€‚
- `mode` (str, å¯é€‰): æ ‡è®°æ¨¡å¼ï¼Œå¯ä» 'all', 'precise' æˆ– 'search_engine' ä¸­é€‰æ‹©ã€‚é»˜è®¤ä¸º 'precise'ã€‚
- `stopwords` (str, å¯é€‰): æŒ‡ç¤ºè¦ä½¿ç”¨çš„åœç”¨è¯æ–‡ä»¶çš„æ–‡ä»¶åå­—ç¬¦ä¸²ã€‚é»˜è®¤ä¸º 'common_stopwords.txt'ã€‚
- `text_only` (bool, å¯é€‰): å¦‚æœä¸º Trueï¼Œåˆ™ä»…æ ‡è®°è‹±æ–‡å’Œä¸­æ–‡æ–‡æœ¬ã€‚é»˜è®¤ä¸º Falseã€‚
- `include_numbers` (bool, å¯é€‰): å¦‚æœä¸º Trueï¼Œåˆ™åœ¨æ ‡è®°çš„è¾“å‡ºä¸­åŒ…å«æ•°å­—ã€‚é»˜è®¤ä¸º Trueã€‚
- `custom_stopwords` (str åˆ—è¡¨, å¯é€‰): è¦åˆ é™¤çš„è‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨ã€‚é»˜è®¤ä¸º Noneã€‚
- `exclude_default_stopwords` (bool, å¯é€‰): å¦‚æœä¸º Trueï¼Œåˆ™æ’é™¤é»˜è®¤çš„åœç”¨è¯ã€‚é»˜è®¤ä¸º Falseã€‚

#### è¿”å›ï¼š
- `list`: æ ¹æ®æŒ‡å®šçš„å‚æ•°åˆ é™¤åœç”¨è¯åçš„æ ‡è®°åˆ—è¡¨ã€‚

#### ç¤ºä¾‹ 1ï¼š
```python
from HanziNLP import word_tokenize
 
sample = 'é™¤éä½ ä¸è¯´ï¼Œæˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello you#$@#@*' # ä¸€ä¸ªæ•…æ„ç”¨äºæ ‡è®°åŒ–çš„å›°éš¾æ–‡æœ¬
token = sz.word_tokenize(sample, mode='precise', stopwords='baidu_stopwords.txt', text_only=False, 
                  include_numbers=True, custom_stopwords=None, exclude_default_stopwords=False)
token
```
#### è¾“å‡º 
```python
['ä¸', 'è¯´', 'ï¼Œ', 'ä¼š', 'å¾ˆ', 'å¼€å¿ƒ', ',', '#', '$', '@', '#', '@', '*']
```
#### ç¤ºä¾‹ 2ï¼šå°† text_only è®¾ç½®ä¸º True å¹¶å°† custom_stopwords è®¾ç½®ä¸º ['å¼€å¿ƒ']
```python
from HanziNLP import word_tokenize

sample = 'é™¤éä½ ä¸è¯´ï¼Œæˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello you#$@#@*'# ä¸€ä¸ªæ•…æ„ç”¨äºæ ‡è®°åŒ–çš„å›°éš¾æ–‡æœ¬
token = sz.word_tokenize(sample, mode='precise', stopwords='baidu_stopwords.txt', text_only=True, 
                  include_numbers=True, custom_stopwords=['å¼€å¿ƒ'], exclude_default_stopwords=False)
token
```
#### è¾“å‡ºï¼šå·²åˆ é™¤ç‰¹æ®Šå­—ç¬¦å’Œå•è¯ 'å¼€å¿ƒ'
```python
['ä¸', 'è¯´', 'ä¼š', 'å¾ˆ']
```
## 5. æ–‡æœ¬è¡¨ç¤º
æ„å»ºæ–‡æœ¬ç‰¹å¾å›¾æ˜¯å„ç§æœºå™¨å­¦ä¹ æˆ–æ·±åº¦å­¦ä¹ ä»»åŠ¡çš„èµ·ç‚¹ã€‚HanziNLPå·²æ•´åˆäº†å¯ä»¥è½»æ¾å®ç°çš„å¸¸è§ç‰¹å¾å›¾æ–¹æ³•ã€‚

### 5.1 è¯è¢‹æ¨¡å‹ (BoW)

- **å‡½æ•°**ï¼š`BoW(segmented_text_list)`
- **ç›®çš„**ï¼šä»ä¸€ç³»åˆ—åˆ†æ®µæ–‡æœ¬ä¸­ç”Ÿæˆè¯è¢‹æ¨¡å‹è¡¨ç¤ºã€‚
- **å‚æ•°**ï¼š
  - `segmented_text_list` (str åˆ—è¡¨)ï¼šåŒ…å«åˆ†æ®µæ–‡æœ¬çš„åˆ—è¡¨ã€‚
- **è¿”å›**ï¼š 
  - `dict`ï¼šè¡¨ç¤ºè¯é¢‘çš„å­—å…¸ã€‚

#### ç¤ºä¾‹
```python
from HanziNLP import word_tokenize, BoW

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
token = word_tokenize(sample_sentence, text_only = True)
bow = BoW(token)
bow
```
#### è¾“å‡º 
```python
{'hello': 2, 'world': 1, 'This': 1, 'Sam': 1, 'è¯´': 1, 'ä»Šå¤©': 1, 'ä¼š': 1, 'å¼€å¿ƒ': 1}
```

### 5.2 ngrams

- **å‡½æ•°**ï¼š`ngrams(tokens, n=3)`
- **ç›®çš„**ï¼šä»æ ‡è®°åˆ—è¡¨ä¸­åˆ›å»ºå¹¶è®¡ç®—n-gramsçš„é¢‘ç‡ã€‚
- **å‚æ•°**ï¼š
  - `tokens` (åˆ—è¡¨)ï¼šæ ‡è®°åˆ—è¡¨ã€‚
  - `n` (int, å¯é€‰)ï¼šn-gramsçš„æ•°å­—ã€‚é»˜è®¤ä¸º3ï¼ˆtrigramsï¼‰ã€‚
- **è¿”å›**ï¼š 
  - `dict`ï¼šä»¥n-gramsä¸ºé”®ï¼Œå…¶é¢‘ç‡ä¸ºå€¼çš„å­—å…¸ã€‚

#### ç¤ºä¾‹
```python
from HanziNLP import word_tokenize, ngrams

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
token = word_tokenize(sample_sentence, text_only = True)
ngram = ngrams(token, n =3)
ngram
```
#### è¾“å‡º 
```python
{'hello world This': 1,
 'world This Sam': 1,
 'This Sam è¯´': 1,
 'Sam è¯´ ä»Šå¤©': 1,
 'è¯´ ä»Šå¤© ä¼š': 1,
 'ä»Šå¤© ä¼š å¼€å¿ƒ': 1,
 'ä¼š å¼€å¿ƒ hello': 1}
```

### 5.3 TF_IDF (è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡)

- **å‡½æ•°**ï¼š`TF_IDF(text_list, max_features=None, output_format='sparse')`
- **ç›®çš„**ï¼šå°†æ–‡æœ¬åˆ—è¡¨è½¬æ¢ä¸ºTF-IDFè¡¨ç¤ºã€‚
- **å‚æ•°**ï¼š
  - `text_list` (str åˆ—è¡¨)ï¼šè¦è½¬æ¢çš„æ ‡è®°åˆ—è¡¨ã€‚
  - `max_features` (int, å¯é€‰)ï¼šè¦æå–çš„æœ€å¤§ç‰¹å¾ï¼ˆæœ¯è¯­ï¼‰æ•°é‡ã€‚é»˜è®¤ä¸ºNoneï¼ˆæ‰€æœ‰ç‰¹å¾ï¼‰ã€‚
  - `output_format` (str, å¯é€‰)ï¼šè¾“å‡ºçŸ©é˜µçš„æ ¼å¼ï¼ˆ'sparse'ï¼Œ'dense' æˆ– 'dataframe'ï¼‰ã€‚é»˜è®¤ä¸º'sparse'ã€‚
- **è¿”å›**ï¼š 
  - `matrix`ï¼šæŒ‡å®šæ ¼å¼çš„TF-IDFçŸ©é˜µã€‚
  - `feature_names`ï¼šç‰¹å¾åç§°åˆ—è¡¨ã€‚

#### ç¤ºä¾‹
```python
from HanziNLP import word_tokenize, TF_IDF

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
token = word_tokenize(sample_sentence, text_only = True)
tfidf_matrix, feature_names = sz.TF_IDF(token, output_format = 'dataframe')
tfidf_matrix
```
#### è¾“å‡º 
![ç¤ºä¾‹å›¾ç‰‡](README_PIC/TFIDF.png)

### 5.4 TT_matrix (è¯-è¯çŸ©é˜µ)

- **å‡½æ•°**ï¼š`TT_matrix(tokenized_texts, window_size=1)`
- **ç›®çš„**ï¼šä»æ ‡è®°æ–‡æœ¬åˆ—è¡¨ç”Ÿæˆæœ¯è¯­-æœ¯è¯­çŸ©é˜µï¼Œè¡¨ç¤ºæŒ‡å®šçª—å£å†…çš„æœ¯è¯­å…±ç°ã€‚
- **å‚æ•°**ï¼š
  - `tokenized_texts` (str åˆ—è¡¨çš„åˆ—è¡¨)ï¼šæ ‡è®°æ–‡æœ¬çš„åˆ—è¡¨ã€‚
  - `window_size` (int)ï¼šå…±ç°çš„çª—å£å¤§å°ã€‚é»˜è®¤ä¸º1ã€‚
- **è¿”å›**ï¼š 
  - `np.array`ï¼šä¸€ä¸ªæ–¹é˜µï¼Œå…¶ä¸­æ¡ç›®ï¼ˆiï¼Œjï¼‰æ˜¯æœ¯è¯­iå’Œæœ¯è¯­jä¹‹é—´çš„å…±ç°ã€‚
  - `index_to_term`ï¼šä»ç´¢å¼•åˆ°æœ¯è¯­çš„å­—å…¸ã€‚

#### ç¤ºä¾‹
```python
from HanziNLP import word_tokenize, TT_matrix

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
token = word_tokenize(sample_sentence, text_only = True)
matrix, index_to_term = TT_matrix(token, window_size = 1)
matrix
```
#### è¾“å‡º 
``` python
array([[0., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [4., 0., 0., 0., 0., 0., 0., 2., 2., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [4., 0., 4., 4., 0., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 4., 0., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 2., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 2., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        2.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.,
        0.]])
```

## 6. æ–‡æœ¬ç›¸ä¼¼æ€§

### text_similarity å‡½æ•°
- **å‡½æ•°**: `text_similarity(text1, text2, method='cosine')`
- **ç›®çš„**: ä½¿ç”¨æŒ‡å®šçš„æ–¹æ³•è®¡ç®—å¹¶è¿”å›ä¸¤ä¸ªè¾“å…¥æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚
- **å‚æ•°**:
  - `text1` (str): ç”¨äºæ¯”è¾ƒçš„ç¬¬ä¸€ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²ã€‚
  - `text2` (str): ç”¨äºæ¯”è¾ƒçš„ç¬¬äºŒä¸ªæ–‡æœ¬å­—ç¬¦ä¸²ã€‚
  - `method` (str): ç”¨äºè®¡ç®—ç›¸ä¼¼åº¦çš„æ–¹æ³•ã€‚é€‰é¡¹åŒ…æ‹¬ 'cosine'ã€'jaccard'ã€'euclidean' æˆ– 'levenshtein'ã€‚é»˜è®¤ä¸º 'cosine'ã€‚
- **è¿”å›**: 
  - `float`: è¡¨ç¤º `text1` å’Œ `text2` ä¹‹é—´ç›¸ä¼¼åº¦åˆ†æ•°çš„æ•°å€¼ã€‚

#### æ¦‚è¿°

`text_similarity` å‡½æ•°ç²¾å¿ƒåˆ¶ä½œï¼Œç”¨äºä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ–¹æ³•è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²ï¼ˆå³ `text1` å’Œ `text2`ï¼‰ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚é¦–å…ˆï¼Œå‡½æ•°å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶å°†å…¶è½¬æ¢ä¸ºå‘é‡å½¢å¼ã€‚éšåï¼Œå®ƒæ ¹æ®æ‰€é€‰æ–¹æ³•è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'cosine'ã€'jaccard'ã€'euclidean' æˆ– 'levenshtein'ã€‚

- **ä½™å¼¦ç›¸ä¼¼åº¦**: æµ‹é‡ä¸¤ä¸ªéé›¶å‘é‡ä¹‹é—´è§’åº¦çš„ä½™å¼¦ï¼Œæä¾›å®ƒä»¬ä¹‹é—´è§’åº¦çš„ä½™å¼¦åº¦é‡ã€‚
- **Jaccard ç›¸ä¼¼åº¦**: è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²çš„äº¤é›†å¤§å°é™¤ä»¥å®ƒä»¬çš„å¹¶é›†å¤§å°ã€‚
- **æ¬§å‡ é‡Œå¾—ç›¸ä¼¼åº¦**: åˆ©ç”¨ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»æ¥è®¡ç®—ç›¸ä¼¼åº¦ã€‚
- **è±æ–‡æ–¯å¦ç›¸ä¼¼åº¦**: ä½¿ç”¨ä¸¤ä¸ªå­—ç¬¦ä¸²ä¹‹é—´çš„è±æ–‡æ–¯å¦è·ç¦»ï¼ˆæˆ–â€œç¼–è¾‘è·ç¦»â€ï¼‰ï¼Œæ ‡å‡†åŒ–ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ã€‚

#### ç¤ºä¾‹ 1ï¼šJaccard ç›¸ä¼¼åº¦
```python
from HanziNLP import text_similarity

sample='ä½ å¥½ä¸–ç•Œ'
sample1 = 'ä½ å¥½ä¸–ç•Œï¼Œhello world'
text_similarity(sample, sample1, method = 'jaccard')
```
#### è¾“å‡º 
```python
0.5
```

#### ç¤ºä¾‹ 2ï¼šè±æ–‡æ–¯å¦ç›¸ä¼¼åº¦
```python
from HanziNLP import text_similarity

sample='ä½ å¥½ä¸–ç•Œ'
sample1 = 'ä½ å¥½ä¸–ç•Œï¼Œhello world'
text_similarity(sample, sample1, method = 'levenshtein')
```
#### è¾“å‡º 
```python
0.07692307692307693
```

## 7. è¯åµŒå…¥

### 7.1 Word2Vec 
- `Word2Vec`: ä½¿ç”¨ FastText æ¨¡å‹è·å–è¯åµŒå…¥ã€‚
- **å‡½æ•°**: `Word2Vec(text, dimension=300)`
- **ç›®çš„**: ä½¿ç”¨é¢„è®­ç»ƒçš„ FastText æ¨¡å‹ä¸ºå¯èƒ½åŒ…å«è‹±æ–‡å’Œä¸­æ–‡å•è¯çš„æ–‡æœ¬è·å–è¯åµŒå…¥ã€‚
- **å‚æ•°**:
  - `text` (str): å¯èƒ½åŒ…å«è‹±æ–‡å’Œä¸­æ–‡å•è¯çš„è¾“å…¥æ–‡æœ¬ã€‚
  - `dimension` (int): ç”Ÿæˆçš„è¯åµŒå…¥çš„ç»´åº¦ã€‚é»˜è®¤å€¼ä¸º 300ã€‚
- **è¿”å›**: 
  - `list of numpy.ndarray`: ä¸€ä¸ªåŒ…å«è¾“å…¥æ–‡æœ¬ä¸­æ¯ä¸ªå•è¯çš„è¯åµŒå…¥çš„åˆ—è¡¨ã€‚

#### æ¦‚è¿°

`Word2Vec` å‡½æ•°æ—¨åœ¨ä½¿ç”¨é¢„è®­ç»ƒçš„ FastText æ¨¡å‹ä¸ºç»™å®šæ–‡æœ¬ç”Ÿæˆè¯åµŒå…¥ï¼Œè¯¥æ–‡æœ¬å¯èƒ½åŒ…å«æ¥è‡ªè‹±æ–‡å’Œä¸­æ–‡çš„å•è¯ã€‚é¦–å…ˆï¼Œå‡½æ•°æ£€æŸ¥å¹¶ä¸‹è½½è‹±æ–‡å’Œä¸­æ–‡çš„ FastText æ¨¡å‹ï¼ˆå¦‚æœå°šæœªä¸‹è½½ï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œå®ƒåŠ è½½æ¨¡å‹ï¼Œå¹¶åœ¨è¯·æ±‚æ—¶å°†å…¶ç»´åº¦å‡å°åˆ°æŒ‡å®šçš„å¤§å°ã€‚

æ–‡æœ¬è¢«åˆ†è¯æˆå•è¯ï¼Œå¯¹äºæ¯ä¸ªå•è¯ï¼Œå‡½æ•°æ£€æŸ¥å®ƒæ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦ã€‚å¦‚æœä¸€ä¸ªå•è¯åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œåˆ™ä½¿ç”¨ä¸­æ–‡ FastText æ¨¡å‹è·å–å…¶åµŒå…¥ï¼›å¦åˆ™ï¼Œä½¿ç”¨è‹±æ–‡æ¨¡å‹ã€‚ç”Ÿæˆçš„åµŒå…¥è¢«è¿½åŠ åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œç„¶åè¿”å›è¯¥åˆ—è¡¨ã€‚

- **ä½¿ç”¨ FastText**: ä½¿ç”¨åœ¨å¤§é‡æ–‡æœ¬è¯­æ–™ä¸Šé¢„è®­ç»ƒçš„ FastText æ¨¡å‹ç”Ÿæˆè¯åµŒå…¥ã€‚
- **æ”¯æŒå¤šç§è¯­è¨€**: ä¸“ä¸ºå¤„ç†åŒ…å«è‹±æ–‡å’Œä¸­æ–‡å•è¯çš„æ–‡æœ¬è€Œè®¾è®¡ï¼Œé€šè¿‡ä½¿ç”¨å„è‡ªçš„è¯­è¨€æ¨¡å‹ã€‚
- **é™ç»´**: æä¾›å°†åµŒå…¥çš„ç»´åº¦å‡å°åˆ°æ‰€éœ€å¤§å°çš„çµæ´»æ€§ã€‚

#### ç¤ºä¾‹
```python
from HanziNLP import Word2Vec

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
result = Word2Vec(sample_sentence)
```

### 7.2 BERT åµŒå…¥
- **å‡½æ•°**: `get_bert_embeddings(text, model="bert-base-chinese")`
- **ç›®çš„**: ä½¿ç”¨é¢„è®­ç»ƒçš„ä¸­æ–‡ BERT æ¨¡å‹ä¸ºæŒ‡å®šæ–‡æœ¬æ£€ç´¢ BERT åµŒå…¥ã€‚
- **å‚æ•°**:
  - `text` (str): éœ€è¦ç”ŸæˆåµŒå…¥çš„è¾“å…¥æ–‡æœ¬ã€‚
  - `model` (str): å°†è¦ä½¿ç”¨çš„é¢„è®­ç»ƒä¸­æ–‡ BERT æ¨¡å‹çš„åç§°ã€‚é»˜è®¤ä¸º "bert-base-chinese"ã€‚
- **è¿”å›**: 
  - `sentence_embedding` (list): è¡¨ç¤ºä¸ºæµ®ç‚¹æ•°åˆ—è¡¨çš„å¥å­åµŒå…¥ã€‚
  - `tokens` (list): ä¸å¥å­åµŒå…¥ç›¸å…³è”çš„ä»¤ç‰Œã€‚

#### æ¦‚è¿°

`get_bert_embeddings` å‡½æ•°æ—¨åœ¨ä½¿ç”¨æŒ‡å®šçš„é¢„è®­ç»ƒä¸­æ–‡ BERT æ¨¡å‹ä¸ºç»™å®šæ–‡æœ¬æå– BERT åµŒå…¥ã€‚é¦–å…ˆï¼Œå‡½æ•°åŠ è½½æŒ‡å®šçš„ BERT æ¨¡å‹åŠå…¶ç›¸åº”çš„åˆ†è¯å™¨ã€‚è¾“å…¥æ–‡æœ¬è¢«åˆ†è¯å¹¶ä¸ºæ¨¡å‹å‡†å¤‡å¥½ï¼Œç¡®ä¿å®ƒè¢«æˆªæ–­ä¸ºæœ€å¤š 512 ä¸ªä»¤ç‰Œï¼Œä»¥ä¾¿ä¸ BERT æ¨¡å‹å…¼å®¹ã€‚

åœ¨åˆ†è¯ä¹‹åï¼Œæ¨¡å‹ç”Ÿæˆé¢„æµ‹ï¼Œå¹¶æ£€ç´¢ BERT æ¨¡å‹çš„æœ€åéšè—çŠ¶æ€ã€‚é€šè¿‡å–æœ€åéšè—çŠ¶æ€çš„å¹³å‡å€¼å¹¶å°†å…¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°åˆ—è¡¨æ¥è®¡ç®—å¥å­åµŒå…¥ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†è¾“å…¥ ID è½¬æ¢å›ä»¤ç‰Œæ¥è·å–ä¸å¥å­åµŒå…¥å…³è”çš„ä»¤ç‰Œã€‚

- **åˆ©ç”¨ BERT**: åˆ©ç”¨é¢„è®­ç»ƒçš„ BERT æ¨¡å‹ï¼Œå› å…¶åœ¨ç”Ÿæˆä¸Šä¸‹æ–‡åµŒå…¥æ–¹é¢çš„æ•ˆæœè€Œé—»åã€‚
- **æ”¯æŒä¸­æ–‡æ–‡æœ¬**: é€šè¿‡ä½¿ç”¨ä¸­æ–‡ BERT æ¨¡å‹ä¸“é—¨å¤„ç†ä¸­æ–‡æ–‡æœ¬ã€‚
- **ä»¤ç‰Œå¤„ç†**: ç¡®ä¿ä»¤ç‰Œè¢«é€‚å½“ç®¡ç†ï¼Œå¹¶ä¸åµŒå…¥ä¸€èµ·è¿”å›ï¼Œä»¥ä¾›å‚è€ƒå’Œè¿›ä¸€æ­¥åˆ†æã€‚

#### ç¤ºä¾‹
```python
from HanziNLP import get_bert_embeddings

embeddings, tokens = get_bert_embeddings(text, model = "bert-base-chinese") # è¾“å…¥ä½ å¸Œæœ›ä½¿ç”¨çš„ Hugging Face çš„ BERT æ¨¡å‹åç§°
print(f"Tokens: {tokens}")
print(f"Embeddings: {embeddings}")
```

## 8. ä¸»é¢˜å»ºæ¨¡
HanziNLP å·²é›†æˆäº†ä»£ç ï¼Œä»¥ä¾¿è½»æ¾å®ç° LDA æ¨¡å‹ï¼Œä»å¤§é‡æ–‡æœ¬ä¸­æå–ä¸»é¢˜ã€‚å°†æ›´æ–°æ›´å¤šæ¨¡å‹ï¼š

### 8.1 æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é… (LDA) æ¨¡å‹

- **å‡½æ•°**: `lda_model(texts, num_topics=10, passes=15, dictionary=None)`
- **ç›®çš„**: åœ¨æä¾›çš„æ–‡æœ¬ä¸Šè®­ç»ƒä¸€ä¸ªæ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é… (LDA) æ¨¡å‹ï¼Œä»¥æå–å’Œè¯†åˆ«ä¸»é¢˜ã€‚
- **å‚æ•°**:
  - `texts` (list of list of str): æ–‡æ¡£çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£è¡¨ç¤ºä¸ºä¸€ä¸ªä»¤ç‰Œåˆ—è¡¨ã€‚
  - `num_topics` (int): è¦æå–çš„ä¸»é¢˜æ•°é‡ã€‚é»˜è®¤ä¸º 10ã€‚
  - `passes` (int): é€šè¿‡è¯­æ–™åº“çš„è®­ç»ƒæ¬¡æ•°ã€‚é»˜è®¤ä¸º 15ã€‚
  - `dictionary` (corpora.Dictionary, å¯é€‰): ä¸€ä¸ªå¯é€‰çš„é¢„è®¡ç®— Gensim å­—å…¸ã€‚
- **è¿”å›**: 
  - `lda_model`: è®­ç»ƒè¿‡çš„ LDA æ¨¡å‹ã€‚
  - `corpus`: ç”¨äºè®­ç»ƒæ¨¡å‹çš„è¯­æ–™åº“ã€‚
  - `dictionary`: ç”¨äºè®­ç»ƒæ¨¡å‹çš„å­—å…¸ã€‚

#### æ¦‚è¿°

`lda_model` å‡½æ•°æ—¨åœ¨åœ¨æ–‡æœ¬é›†åˆä¸Šè®­ç»ƒä¸€ä¸ª LDA æ¨¡å‹ï¼Œä¾¿äºæå–å’Œè¯†åˆ«æ½œåœ¨çš„ä¸»é¢˜ã€‚å¦‚æœæ²¡æœ‰æä¾›é¢„è®¡ç®—çš„å­—å…¸ï¼Œå‡½æ•°ä¼šä»è¾“å…¥æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªæ–°çš„å­—å…¸ã€‚æ–‡æœ¬è¢«è½¬æ¢ä¸ºè¯è¢‹è¡¨ç¤ºå½¢å¼ï¼ŒLDA æ¨¡å‹ä½¿ç”¨æŒ‡å®šæˆ–é»˜è®¤å‚æ•°è¿›è¡Œè®­ç»ƒã€‚è¿”å›è®­ç»ƒè¿‡çš„æ¨¡å‹ã€è¯­æ–™åº“å’Œå­—å…¸ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥åˆ†æå’Œä¸»é¢˜å¯è§†åŒ–ã€‚

- **ä¸»é¢˜å»ºæ¨¡**: åˆ©ç”¨ LDAï¼Œä¸€ç§æµè¡Œçš„ä¸»é¢˜å»ºæ¨¡æŠ€æœ¯ï¼Œæ­ç¤ºæ–‡æœ¬æ•°æ®ä¸­çš„æ½œåœ¨ä¸»é¢˜ã€‚
- **çµæ´»çš„è®­ç»ƒ**: å…è®¸æŒ‡å®šä¸»é¢˜æ•°é‡ã€è®­ç»ƒæ¬¡æ•°å’Œï¼ˆå¯é€‰çš„ï¼‰é¢„è®¡ç®—å­—å…¸ã€‚
- **é€‚ç”¨æ€§**: é€‚ç”¨äºåˆ†æå¤§é‡æ–‡æœ¬æ•°æ®ï¼Œä»¥å‘ç°ä¸»é¢˜ç»“æ„ã€‚

### 8.2 LDA print_topics å‡½æ•°

- **å‡½æ•°**: `print_topics(lda_model, num_words=10)`
- **ç›®çš„**: æ˜¾ç¤ºæ¥è‡ªè®­ç»ƒè¿‡çš„ LDA æ¨¡å‹çš„æ¯ä¸ªä¸»é¢˜çš„å‰å‡ ä¸ªå•è¯ã€‚
- **å‚æ•°**:
  - `lda_model`: è®­ç»ƒè¿‡çš„ LDA æ¨¡å‹ã€‚
  - `num_words` (int): æ¯ä¸ªä¸»é¢˜è¦æ˜¾ç¤ºçš„å‰å‡ ä¸ªå•è¯ã€‚é»˜è®¤ä¸º 10ã€‚
- **è¿”å›**: 
  - æ— ï¼ˆè¾“å‡ºæ‰“å°åˆ°æ§åˆ¶å°ï¼‰ã€‚

#### æ¦‚è¿°

`print_topics` å‡½æ•°æ—¨åœ¨æ˜¾ç¤ºæ¥è‡ªè®­ç»ƒè¿‡çš„ LDA æ¨¡å‹çš„æ¯ä¸ªä¸»é¢˜çš„å‰å‡ ä¸ªå•è¯ï¼Œæä¾›äº†æ¯ä¸ªä¸»é¢˜çš„ä¸»é¢˜å®è´¨çš„å¿«é€Ÿè€Œæœ‰è§åœ°çš„æ¦‚è§ˆã€‚é€šè¿‡è¿­ä»£æ¯ä¸ªä¸»é¢˜ï¼Œå®ƒæ‰“å°ä¸»é¢˜ç´¢å¼•å’Œå‰å‡ ä¸ªå•è¯ï¼Œå¸®åŠ©è§£é‡Šå’Œåˆ†æ LDA æ¨¡å‹æå–çš„ä¸»é¢˜ã€‚

- **ä¸»é¢˜è§£é‡Š**: ä¾¿äºè½»æ¾è§£é‡Š LDA æ¨¡å‹ç”Ÿæˆçš„ä¸»é¢˜ã€‚
- **è‡ªå®šä¹‰è¾“å‡º**: å…è®¸ç”¨æˆ·æŒ‡å®šæ¯ä¸ªä¸»é¢˜è¦æ˜¾ç¤ºçš„å‰å‡ ä¸ªå•è¯ã€‚
- **æœ‰è§åœ°çš„æ¦‚è§ˆ**: æä¾›äº†æ–‡æœ¬æ•°æ®ä¸­ä¸»è¦ä¸»é¢˜çš„ç®€æ´è€Œä¿¡æ¯ä¸°å¯Œçš„æ¦‚è§ˆã€‚

#### ç¤ºä¾‹
```python
from HanziNLP import sentence_segment, word_tokenize, lda_model, print_topics

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
sentences = sentence_segment(sample_sentence)
tokenized_texts = [word_tokenize(sentence) for sentence in sentences]
lda_model, corpus, dictionary = lda_model(tokenized_texts, num_topics=5)
print_topics(lda_model)
```
#### è¾“å‡º
```python
Topic: 0 
Words: 0.231*"This" + 0.231*"Sam" + 0.231*"." + 0.038*"è¯´" + 0.038*"hello" + 0.038*"world" + 0.038*"!" + 0.038*"ä»Šå¤©" + 0.038*"å¼€å¿ƒ" + 0.038*"ä¼š"
Topic: 1 
Words: 0.231*"world" + 0.231*"!" + 0.231*"hello" + 0.038*"è¯´" + 0.038*"." + 0.038*"Sam" + 0.038*"This" + 0.038*"ä»Šå¤©" + 0.038*"ä¼š" + 0.038*"å¼€å¿ƒ"
Topic: 2 
Words: 0.091*"è¯´" + 0.091*"This" + 0.091*"!" + 0.091*"hello" + 0.091*"." + 0.091*"world" + 0.091*"Sam" + 0.091*"å¼€å¿ƒ" + 0.091*"ä»Šå¤©" + 0.091*"ä¼š"
Topic: 3 
Words: 0.146*"." + 0.146*"hello" + 0.146*"," + 0.146*"ä¼š" + 0.146*"å¼€å¿ƒ" + 0.146*"ä»Šå¤©" + 0.024*"è¯´" + 0.024*"Sam" + 0.024*"!" + 0.024*"world"
Topic: 4 
Words: 0.375*"è¯´" + 0.063*"hello" + 0.063*"." + 0.063*"!" + 0.063*"Sam" + 0.063*"world" + 0.063*"This" + 0.063*"ä»Šå¤©" + 0.063*"ä¼š" + 0.063*"å¼€å¿ƒ"
```

## 9. æƒ…æ„Ÿåˆ†æ
æƒ…æ„Ÿåˆ†æåœ¨ NLP ä»»åŠ¡ä¸­å¾ˆå¸¸è§ï¼Œæ–‡æœ¬çš„æƒ…æ„Ÿå¯ä»¥ä¸ºè¿›ä¸€æ­¥çš„ç ”ç©¶åˆ†æåšå‡ºè´¡çŒ®ã€‚è™½ç„¶æœ‰è®¸å¤šæ–¹æ³•å¯ä»¥è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œæ¯”å¦‚ä½¿ç”¨æƒ…æ„Ÿè¯å…¸ï¼Œä½†HanziNLPé›†æˆäº†å‡½æ•°ï¼Œå…è®¸è½»æ¾ä½¿ç”¨é¢„è®­ç»ƒçš„ BERT æ¨¡å‹æˆ– Huggin Face ä¸Šçš„å…¶ä»–è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚

### sentiment å‡½æ•°

- **å‡½æ•°**: `sentiment(text, model='hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2', print_all=True, show=False)`
- **ç›®çš„**: ä½¿ç”¨æŒ‡å®šçš„é¢„è®­ç»ƒæ¨¡å‹å¯¹è¾“å…¥æ–‡æœ¬æ‰§è¡Œæƒ…æ„Ÿåˆ†æï¼Œå¹¶å¯é€‰æ‹©æ€§åœ°å¯è§†åŒ–æƒ…æ„Ÿæ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒã€‚
- **å‚æ•°**:
  - `text` (str): ç”¨äºæƒ…æ„Ÿåˆ†æçš„è¾“å…¥æ–‡æœ¬ã€‚
  - `model` (str): è¦ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹çš„æ ‡è¯†ç¬¦ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ **Hugging Face** ä¸Šçš„ä»»ä½•æ¨¡å‹ï¼Œå¹¶åœ¨æ­¤å¤„å¤åˆ¶æ¨¡å‹åç§°ä»¥ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ã€‚é»˜è®¤ä¸º 'hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2'ã€‚
  - `print_all` (bool): æŒ‡ç¤ºæ˜¯å¦æ‰“å°æ‰€æœ‰æ ‡ç­¾çš„æ¦‚ç‡ï¼Œæˆ–ä»…æ‰“å°æ¦‚ç‡æœ€é«˜çš„æ ‡ç­¾ã€‚é»˜è®¤ä¸º Trueã€‚
  - `show` (bool): æŒ‡ç¤ºæ˜¯å¦æ˜¾ç¤ºæ˜¾ç¤ºæ ‡ç­¾æ¦‚ç‡åˆ†å¸ƒçš„æ¡å½¢å›¾ã€‚é»˜è®¤ä¸º Falseã€‚
- **è¿”å›**: 
  - `dict` æˆ– `tuple`: å¦‚æœ `print_all` ä¸º Trueï¼Œåˆ™è¿”å›ä¸€ä¸ªåŒ…å«æƒ…æ„Ÿæ ‡ç­¾åŠå…¶ç›¸åº”æ¦‚ç‡çš„å­—å…¸ã€‚å¦‚æœ `print_all` ä¸º Falseï¼Œåˆ™è¿”å›ä¸€ä¸ªåŒ…å«æ¦‚ç‡æœ€é«˜çš„æ ‡ç­¾åŠå…¶ç›¸åº”æ¦‚ç‡çš„å…ƒç»„ã€‚

#### æ¦‚è¿°

`sentiment` å‡½æ•°ä¸“ä¸ºä½¿ç”¨æŒ‡å®šçš„é¢„è®­ç»ƒæ¨¡å‹å¯¹æä¾›çš„æ–‡æœ¬æ‰§è¡Œæƒ…æ„Ÿåˆ†æè€Œå®šåˆ¶ã€‚åœ¨åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹åï¼Œè¾“å…¥æ–‡æœ¬è¢«åˆ†è¯å¹¶ä¼ é€’ç»™æ¨¡å‹ä»¥è·å–è¾“å‡º logitsã€‚ç„¶åä½¿ç”¨ softmax å‡½æ•°å°†è¿™äº› logits è½¬æ¢ä¸ºæ¦‚ç‡ã€‚ä»æ¨¡å‹çš„é…ç½®ä¸­æ£€ç´¢ä¸è¿™äº›æ¦‚ç‡ç›¸å¯¹åº”çš„æ ‡ç­¾ï¼Œå¹¶å°†å®ƒä»¬åŠå…¶å„è‡ªçš„æ¦‚ç‡å­˜å‚¨åœ¨å­—å…¸ä¸­ã€‚

å¦‚æœ `show` è®¾ç½®ä¸º Trueï¼Œåˆ™æ˜¾ç¤ºä¸€ä¸ªæ¡å½¢å›¾ï¼Œå¯è§†åŒ–æƒ…æ„Ÿæ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒã€‚å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æƒ…æ„Ÿæ ‡ç­¾åŠå…¶ç›¸åº”æ¦‚ç‡çš„å­—å…¸ï¼ˆå¦‚æœ `print_all` ä¸º Trueï¼‰ï¼Œæˆ–åŒ…å«æ¦‚ç‡æœ€é«˜çš„æ ‡ç­¾åŠå…¶ç›¸åº”æ¦‚ç‡çš„å…ƒç»„ï¼ˆå¦‚æœ `print_all` ä¸º Falseï¼‰ã€‚

- **æƒ…æ„Ÿåˆ†æ**: åˆ©ç”¨æŒ‡å®šçš„é¢„è®­ç»ƒæ¨¡å‹åˆ†æè¾“å…¥æ–‡æœ¬çš„æƒ…æ„Ÿã€‚
- **å¯è§†åŒ–**: å¯é€‰æ‹©ä½¿ç”¨æ¡å½¢å›¾å¯è§†åŒ–æƒ…æ„Ÿæ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒã€‚
- **çµæ´»çš„è¾“å‡º**: æä¾›çµæ´»çš„è¾“å‡ºï¼Œå…è®¸è¿›è¡Œè¯¦ç»†æˆ–ç®€æ´çš„æƒ…æ„Ÿåˆ†æç»“æœã€‚

#### ç¤ºä¾‹
```python
from HanziNLP import sentiment

text = "è¿™ä¸ªå°å…„å¼Ÿå¼¹çš„å¤ªå¥½äº†"
sentiment_result = sentiment(text, model='touch20032003/xuyuan-trial-sentiment-bert-chinese', show=True)  # åœ¨ Hugging Face ä¸Šè¾“å…¥ä»»ä½•é¢„è®­ç»ƒçš„åˆ†ç±»æ¨¡å‹
print('sentiment =', sentiment_result)
```
#### è¾“å‡º
```python
sentiment = {'none': 2.7154697818332352e-05, 'disgust': 2.6893396352534182e-05, 'happiness': 0.00047770512173883617, 'like': 0.9991452693939209, 'fear': 3.293586996733211e-05, 'sadness': 0.00013537798076868057, 'anger': 8.243478805525228e-05, 'surprise': 7.21854084986262e-05}
```
![ç¤ºä¾‹å›¾ç‰‡](README_PIC/sentiment.png)

## åœ¨æ‚¨çš„ç ”ç©¶ä¸­å¼•ç”¨HanziNLP

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº† **HanziNLP**ï¼Œè¯·è€ƒè™‘æŒ‰ç…§ä»¥ä¸‹æ–¹å¼å¼•ç”¨å®ƒï¼š

### APA æ ¼å¼

Zhan, Shi. (2023). HanziNLP (Version 0.1.0) [Software]. GitHub. [https://github.com/samzshi0529/HanziNLP](https://github.com/samzshi0529/HanziNLP)

### BibTeX æ¡ç›®

å¯¹äº LaTeX æ–‡æ¡£çš„ä½¿ç”¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ BibTeX å¼•ç”¨ï¼š

```bibtex
@misc{Zhan2023,
  author = {Zhan, Shi.},
  title = {HanziNLP},
  year = {2023},
  publisher = {GitHub},
  version = {0.1.0},
  howpublished = {\url{https://github.com/samzshi0529/HanziNLP}}
}
```
</details>

# HanziNLP

An **user-friendly** and **easy-to-use** Natural Language Processing package specifically designed for Chinese text analysis, modeling, and visualization. All functions in HanziNLP supports Chinese text and works well for Chinese text!

If you find HanziNLP helpful, it will be greatly important that you can add a ğŸŒŸ to this repository! Thanks!

## Table of Contents
- [1. Quick Start](#1-quick-start)
  - [1.1 Related Links](#11-related-links)
  - [1.2 Installing and Usage](#12-installing-and-usage)
  - [1.3 Interactive Dashboard](#13-interactive-dashboard)
- [2. Character and Word Counting](#2-character-and-word-counting)
- [3. Font Management](#3-font-management)
- [4. Text Segmentation](#4-text-segmentation)
  - [4.1 Stopword Management](#41-stopword-management)
  - [4.2 Sentence Segmentation](#42-sentence-segmentation)
  - [4.3 Word Tokenization](#43-word-tokenization)
- [5. Text Representation](#5-text-representation)
  - [5.1 BoW (Bag of Words)](#51-bow-bag-of-words)
  - [5.2 ngrams](#52-ngrams)
  - [5.3 TF_IDF (Term Frequency-Inverse Document Frequency)](#53-tf_idf-term-frequency-inverse-document-frequency)
  - [5.4 TT_matrix (Term-Term Matrix)](#54-tt_matrix-term-term-matrix)
- [6. Text Similarity](#6-text-similarity)
- [7. Word Embeddings](#7-word-embeddings)
  - [7.1 Word2Vec](#71-word2vec)
  - [7.2 BERT Embeddings](#72-bert-embeddings)
- [8. Topic Modeling](#8-topic-modeling)
  - [8.1 Latent Dirichlet Allocation (LDA) model](#81-latent-dirichlet-allocation-lda-model)
  - [8.2 LDA print_topics function](#82-lda-print-topics-function)
- [9. Sentiment Analysis](#9-sentiment-analysis)
- [Citation](#citation)

## Developer Note:

To anyone using HanziNLP, big thanks to you from the developer æ–½å±•,Samuel Shi! ğŸ‰ğŸ‰ğŸ‰ 

For any improvement and more information about me, you can find via the following ways:
- **Personal Email**: samzshi@sina.com
- **Personal Webiste**: [https://www.samzshi.com/](https://www.samzshi.com/)
- **Linkedin**: [www.linkedin.com/in/zhanshisamuel](www.linkedin.com/in/zhanshisamuel)

## 1. Quick Start

Welcome to **HanziNLP** ğŸŒŸ - an ready-to-use toolkit for Natural Language Processing (NLP) on Chinese text, while also accommodating English. It is designed to be user-friendly and simplified tool even for freshmen in python like simple way for Chinese word tokenization and visualization via built-in Chinese fonts(Simplified Chinese).

Moreover, HanziNLP features an interactive dashboard for dynamic insights into NLP functionalities, providing a dynamic overview and insights into various NLP functionalities.

### 1.1 Related Links

- **GitHub Repository**: Explore my code and contribute on [GitHub](https://github.com/samzshi0529/HanziNLP).
- **PyPI Page**: Find me on [PyPI](https://libraries.io/pypi/HanziNLP) and explore more about how to integrate HanziNLP into your projects.

### 1.2 Installing and Usage

Getting started with HanziNLP is as simple as executing a single command!

```python
pip install HanziNLP
```

### 1.3 Interactive Dashboard

![Alt Text](README_PIC/dashboard_video.gif)

#### Use the dashboard() by a simple line!

```python
from HanziNLP import dashboard
dashboard()
```

- **Function**: `dashboard()`
- **Purpose**: Present a user-friendly dashboard that facilitates interactive text analysis and sentiment classification, enabling users to observe the impacts of various pre-trained models and tokenization parameters on the processed text and thereby select the optimal model and parameters for their use case.
- **Parameters**: No parameters are required.
- **Returns**: No return value; the function outputs a dashboard interface.

#### Overview

The `dashboard` function introduces a user-interactive dashboard, designed to perform text analysis and sentiment classification, providing users with a hands-on experience to explore and understand the effects of different pre-trained models and tokenization parameters on text processing.

- **Interactive Text Analysis**: Users can input text, observe various text statistics, such as word count, character count, and sentence count, and visualize token frequencies and sentiment classification results.
- **Model Exploration**: Users have the option to specify a classification model from Hugging Face. If left blank, a default model, 'uer/roberta-base-finetuned-chinanews-chinese', is utilized. More about this model can be found on [Hugging Face](https://huggingface.co/uer/roberta-base-finetuned-chinanews-chinese).
- **Tokenization Parameter Tuning**: Users can adjust tokenization settings, such as the 'Jieba Mode' parameter and stopwords selection, and observe the resultant tokens and their respective frequencies.
- **Visualization**: The dashboard provides visual insights into text statistics, word frequencies, and sentiment classification, aiding users in understanding the text analysis results.
- **Sentiment Classification**: The dashboard performs sentiment classification using the specified (or default) model and displays the probability distribution across sentiment labels.

#### Highlight

The `dashboard` function emphasizes **user engagement** and **exploration**. It allows users to interactively engage with various pre-trained models and tokenization parameters, observing their effects on text analysis and sentiment classification. This interactive exploration enables users to make informed decisions, selecting the model and parameters that best align with their specific use case, thereby enhancing their text analysis and natural language processing (NLP) tasks.

## 2. Character and Word Counting

ğŸš€ This basic function count the characters and words in your text, sparing you the manual effot of identifying and splitting Chinese words on your own. 

### char_freq and word_freq Functions
- `char_freq(text, text_only=True)`: Function to calculate the frequency of each character in a given text; If text_only == True, only Chinese and English characters will be counted. If text_only == False, all characters will be counted. Default to be True.
- `word_freq(text)`: Function to calculate the frequency of each word in a given text.
### Example
```python
from HanziNLP import char_freq, word_freq

text = "ä½ å¥½, ä¸–ç•Œ!"
char_count = char_freq(text)
word_count = word_freq(text)

print(f"Character Count: {char_count}")
print(f"Word Count: {word_count}")
```
### Output 
```python
Charater Count: 4
Word Count: 2
```
## 3. Font Management

When visualizing Chinese text in Python environment, font is a vital resource which is often needed from manual importing. HanziNLP have built-in list of fonts for usage right away. You can use list_fonts() to see and filter all available fonts and use get_font() to retrieve a specific font path for visualization purposes. All built-in fonts are from Google fonts that are licensed under the Open Font License, meaning one can use them in your products & projects â€“ print or digital, commercial or otherwise.

### list_fonts and get_font Functions
- `list_fonts()`: List all available fonts.
- `get_font(font_name, show=True)`: Retrieve a specific font for visualization purposes. If show == True, a sample visualization of the font will be shown. If show == False, nothing will be shown. Default set to be True.

#### list_fonts() example
```python
from HanziNLP import list_fonts

# List all available fonts
list_fonts()
```
#### output
![Example Image](README_PIC/list_fonts().png)

#### get_font() example
```python
from HanziNLP import get_font

font_path = get_font('ZCOOLXiaoWei-Regular') #Enter the font_name you like in list_fonts()
```
#### output
![Example Image](README_PIC/get_font.png)

#### WordCloud Example
You can use the Chinese font_path you defined to make all kinds of plots. A wordcloud example is provided below:
```python
from PIL import Image
from wordcloud import WordCloud,ImageColorGenerator
import matplotlib.pyplot as plt

# A sample text generated by GPT-4 
text = 'åœ¨æ˜åªšçš„æ˜¥å¤©é‡Œï¼Œå°èŠ±çŒ«å’ªæ‚ é—²åœ°èººåœ¨çª—å°ä¸Šï¼Œäº«å—ç€æ¸©æš–çš„é˜³å…‰ã€‚å¥¹çš„çœ¼ç›é—ªçƒç€å¥½å¥‡çš„å…‰èŠ’ï¼Œæ—¶ä¸æ—¶åœ°è§‚å¯Ÿç€çª—å¤–å¿™ç¢Œçš„å°é¸Ÿå’Œè´è¶ã€‚å°çŒ«çš„å°¾å·´è½»è½»æ‘‡åŠ¨ï¼Œè¡¨è¾¾ç€å¥¹å†…å¿ƒçš„èˆ’é€‚å’Œæ»¡è¶³ã€‚åœ¨å¥¹çš„èº«è¾¹ï¼Œä¸€ç›†ç››å¼€çš„ç´«ç½—å…°æ•£å‘ç€æ·¡æ·¡çš„é¦™æ°”ï¼Œç»™è¿™ä¸ªå®é™çš„åˆåå¢æ·»äº†å‡ åˆ†è¯—æ„ã€‚å°èŠ±çŒ«å’ªå¶å°”ä¼šé—­ä¸Šå¥¹çš„çœ¼ç›ï¼Œæ²‰æµ¸åœ¨è¿™ç¾å¥½çš„æ—¶å…‰ä¸­ï¼Œä»¿ä½›æ•´ä¸ªä¸–ç•Œéƒ½å˜å¾—æ¸©é¦¨å’Œè°ã€‚çª—å¤–çš„æ¨±èŠ±æ ‘åœ¨å¾®é£ä¸­è½»è½»æ‘‡æ›³ï¼Œæ´’ä¸‹ä¸€ç‰‡ç‰‡ç²‰è‰²çš„èŠ±ç“£ï¼Œå¦‚æ¢¦å¦‚å¹»ã€‚åœ¨è¿™æ ·çš„ä¸€ä¸ªæ‚ æ‰˜çš„æ˜¥æ—¥é‡Œï¼Œä¸€åˆ‡éƒ½æ˜¾å¾—å¦‚æ­¤ç¾å¥½å’Œå¹³é™ã€‚'

text = " ".join(text)

# Generate the word cloud
wordcloud = WordCloud(font_path= font_path, width=800, height=800,
                      background_color='white',
                      min_font_size=10).generate(text)

# Display the word cloud
plt.figure(figsize=(5, 5), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.title("sample wordcloud")

plt.show()
```
#### output
![Example Image](README_PIC/wordcloud.png)

## 4. Text Segmentation
Text Segmentatino is a vital step in any NLP tasks. The general step is to segment the sentences, remove stopwords, and tokenize each sentences separately. The detailed instructions are introduced below. 

### 4.1 Stopword Management
To remove stopwords in Chinese text, the package have built-in common stopwords lists include the following ones: (Some stopwords are from [stopwords](https://github.com/goto456/stopwords/))

| Stopword List | File Name |
|----------|----------|
| ä¸­æ–‡åœç”¨è¯è¡¨ | cn_stopwords.txt |
| å“ˆå·¥å¤§åœç”¨è¯è¡¨ | hit_stopwords.txt |
| ç™¾åº¦åœç”¨è¯è¡¨ | baidu_stopwords.txt |
| å››å·å¤§å­¦æœºå™¨æ™ºèƒ½å®éªŒå®¤åœç”¨è¯è¡¨ | scu_stopwords.txt |
| å¸¸ç”¨åœç”¨è¯è¡¨ | common_stopwords.txt |

#### list_stopwords and load_stopwords Functions
- `list_stopwords()`: List all available stopwords.
- `load_stopwords(file_name)`: Load stopwords from a specified file to a list of words. You can then see and use the stopwords for later usage. 

##### list_stopwords example
```python
from HanziNLP import list_stopwords

list_stopwords()
```
##### output 
![Example Image](README_PIC/list_stopwords.png)

##### load_stopwords example
```python
from HanziNLP import load_stopwords

stopwords = load_stopwords('common_stopwords.txt') # Enter the txt file name here
```
##### output 
```python
{'ç„¶è€Œ',
 'whoever',
 'åªé™',
 'çš„ç¡®',
 'è¦ä¸ç„¶',
 'each',
 'ä»æ—§',
 'è¿™ä¹ˆç‚¹å„¿',
 'å†’',
 'å¦‚æœ',
 'æ¯”åŠ',
 'ä»¥æœŸ',
 'çŠ¹è‡ª'.....
}
```

### 4.2 Sentence Segmentation
This function segments a whole document or paragraphs into sentences. Support both Chinese and English text.
- `sentence_segment(text)`: Segment the input text into sentences. 

#### sentence_segment example: This example intentially chooses a hard sentence to split.
```python
from HanziNLP import sentence_segment

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
sentence_segment(sample_sentence)
```
#### output 
```python
['hello world!', 'This is Sam.', 'ã€‚', 'é™¤éä½ ä¸è¯´ã€‚', 'æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .', 'youã€‚']
```

### 4.3 Word Tokenization
As one of the most important step in preprocessing text for NLP tasks, the word_tokenize() function provide a direct way to transform raw Chinese text into tokens. 

- **Function**: `word_tokenize(text, mode='precise', stopwords='common_stopwords.txt', text_only=False, include_numbers=True, custom_stopwords=None, exclude_default_stopwords=False)`
- **Purpose**: Tokenize the input text into words while providing options to manage stopwords effectively.
  
#### Parameters:
- `text` (str): The input Chinese text.
- `mode` (str, optional): Tokenization mode, choose from 'all', 'precise', or 'search_engine'. Default is 'precise'.
- `stopwords` (str, optional): A filename string indicating the stopwords file to be used. Default is 'common_stopwords.txt'.
- `text_only` (bool, optional): If True, only tokenize English and Chinese texts. Default is False.
- `include_numbers` (bool, optional): Include numbers in the tokenized output if True. Default is True.
- `custom_stopwords` (list of str, optional): A list of custom stopwords to be removed. Default is None.
- `exclude_default_stopwords` (bool, optional): Exclude default stopwords if True. Default is False.

#### Returns:
- `list`: A list of tokens, with stopwords removed according to the specified parameters.

#### Example 1ï¼š
```python
from HanziNLP import word_tokenize
 
sample = 'é™¤éä½ ä¸è¯´ï¼Œæˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello you#$@#@*' # A text intentionally to be hard for tokenization
token = sz.word_tokenize(sample, mode='precise', stopwords='baidu_stopwords.txt', text_only=False, 
                  include_numbers=True, custom_stopwords=None, exclude_default_stopwords=False)
token
```
#### output 
```python
['ä¸', 'è¯´', 'ï¼Œ', 'ä¼š', 'å¾ˆ', 'å¼€å¿ƒ', ',', '#', '$', '@', '#', '@', '*']
```
#### Example 2ï¼š set text_only to be True and custom_stopwords to be ['å¼€å¿ƒ']
```python
from HanziNLP import word_tokenize

sample = 'é™¤éä½ ä¸è¯´ï¼Œæˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello you#$@#@*'# A text intentionally to be hard for tokenization
token = sz.word_tokenize(sample, mode='precise', stopwords='baidu_stopwords.txt', text_only=True, 
                  include_numbers=True, custom_stopwords=['å¼€å¿ƒ'], exclude_default_stopwords=False)
token
```
#### output: Special characters and the word 'å¼€å¿ƒ' are removed
```python
['ä¸', 'è¯´', 'ä¼š', 'å¾ˆ']
```

## 5. Text Representation
Building text feature map is the starting point for various Machine Learning or Deep Learning tasks. HanziNLP has incorporate the common feature map methods that can be easily implemented.

### 5.1 BoW (Bag of Words)

- **Function**: `BoW(segmented_text_list)`
- **Purpose**: Generate a Bag of Words representation from a list of segmented texts.
- **Parameters**:
  - `segmented_text_list` (list of str): A list containing segmented texts.
- **Returns**: 
  - `dict`: A dictionary representing word frequencies.

#### example
```python
from HanziNLP import word_tokenize, BoW

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
token = word_tokenize(sample_sentence, text_only = True)
bow = BoW(token)
bow
```
#### output 
```python
{'hello': 2, 'world': 1, 'This': 1, 'Sam': 1, 'è¯´': 1, 'ä»Šå¤©': 1, 'ä¼š': 1, 'å¼€å¿ƒ': 1}
```

### 5.2 ngrams

- **Function**: `ngrams(tokens, n=3)`
- **Purpose**: Create and count the frequency of n-grams from a list of tokens.
- **Parameters**:
  - `tokens` (list): A list of tokens.
  - `n` (int, optional): The number for n-grams. Default is 3 (trigrams).
- **Returns**: 
  - `dict`: A dictionary with n-grams as keys and their frequencies as values.

#### example
```python
from HanziNLP import word_tokenize, ngrams

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
token = word_tokenize(sample_sentence, text_only = True)
ngram = ngrams(token, n =3)
ngram
```
#### output 
```python
{'hello world This': 1,
 'world This Sam': 1,
 'This Sam è¯´': 1,
 'Sam è¯´ ä»Šå¤©': 1,
 'è¯´ ä»Šå¤© ä¼š': 1,
 'ä»Šå¤© ä¼š å¼€å¿ƒ': 1,
 'ä¼š å¼€å¿ƒ hello': 1}
```

### 5.3 TF_IDF (Term Frequency-Inverse Document Frequency)

- **Function**: `TF_IDF(text_list, max_features=None, output_format='sparse')`
- **Purpose**: Transform a list of texts into a TF-IDF representation.
- **Parameters**:
  - `text_list` (list of str): A list of tokens to be transformed.
  - `max_features` (int, optional): Maximum number of features (terms) to be extracted. Defaults to None (all features).
  - `output_format` (str, optional): Format of the output matrix ('sparse', 'dense', or 'dataframe'). Defaults to 'sparse'.
- **Returns**: 
  - `matrix`: TF-IDF matrix in the specified format.
  - `feature_names`: List of feature names.

#### example
```python
from HanziNLP import word_tokenize, TF_IDF

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
token = word_tokenize(sample_sentence, text_only = True)
tfidf_matrix, feature_names = sz.TF_IDF(token, output_format = 'dataframe')
tfidf_matrix
```
#### output 
![Example Image](README_PIC/TFIDF.png)

### 5.4 TT_matrix (Term-Term Matrix)

- **Function**: `TT_matrix(tokenized_texts, window_size=1)`
- **Purpose**: Generate a term-term matrix from a list of tokenized texts, representing term co-occurrences within a specified window.
- **Parameters**:
  - `tokenized_texts` (list of list of str): A list of tokenized texts.
  - `window_size` (int): The window size for co-occurrence. Default is 1.
- **Returns**: 
  - `np.array`: A square matrix where entry (i, j) is the co-occurrence between term i and term j.
  - `index_to_term`: A dictionary mapping from index to term.

#### example
```python
from HanziNLP import word_tokenize, TT_matrix

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
token = word_tokenize(sample_sentence, text_only = True)
matrix, index_to_term = TT_matrix(token, window_size = 1)
matrix
```
#### output 
``` python
array([[0., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [4., 0., 0., 0., 0., 0., 0., 2., 2., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [4., 0., 4., 4., 0., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 4., 0., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 2., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 2., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        2.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.,
        0.]])
```

## 6. Text Similarity

### text_similarity Function
- **Function**: `text_similarity(text1, text2, method='cosine')`
- **Purpose**: To calculate and return the similarity score between two input texts, utilizing a specified method.
- **Parameters**:
  - `text1` (str): The first text string for comparison.
  - `text2` (str): The second text string for comparison.
  - `method` (str): The method utilized for computing similarity. Options include 'cosine', 'jaccard', 'euclidean', or 'levenshtein'. Default is 'cosine'.
- **Returns**: 
  - `float`: A numerical value representing the similarity score between `text1` and `text2`.

#### Overview

The `text_similarity` function is meticulously crafted to calculate the similarity between two text strings, namely `text1` and `text2`, using a method specified by the user. Initially, the function tokenizes the input texts and converts them into vectorized forms. Subsequently, it computes the similarity score based on the chosen method, which can be one of the following: 'cosine', 'jaccard', 'euclidean', or 'levenshtein'.

- **Cosine Similarity**: Measures the cosine of the angle between two non-zero vectors, providing a measure of the cosine of the angle between them.
- **Jaccard Similarity**: Calculates the size of the intersection divided by the size of the union of the two text strings.
- **Euclidean Similarity**: Utilizes the Euclidean distance between two vectors to compute similarity.
- **Levenshtein Similarity**: Employs the Levenshtein distance, or "edit distance", between two strings, normalized to a similarity score.

#### example 1: Jaccard Similarity
```python
from HanziNLP import text_similarity

sample='ä½ å¥½ä¸–ç•Œ'
sample1 = 'ä½ å¥½ä¸–ç•Œï¼Œhello world'
text_similarity(sample, sample1, method = 'jaccard')
```
#### output 
```python
0.5
```

#### example 1: Levenshtein Similarity
```python
from HanziNLP import text_similarity

sample='ä½ å¥½ä¸–ç•Œ'
sample1 = 'ä½ å¥½ä¸–ç•Œï¼Œhello world'
text_similarity(sample, sample1, method = 'levenshtein')
```
#### output 
```python
0.07692307692307693
```

## 7. Word Embeddings

### 7.1 Word2Vec 
- `Word2Vec`: Obtain word embeddings using the FastText model.
- **Function**: `Word2Vec(text, dimension=300)`
- **Purpose**: Obtain word embeddings for a text that may contain both English and Chinese words, utilizing pre-trained FastText models.
- **Parameters**:
  - `text` (str): The input text, which may encompass both English and Chinese words.
  - `dimension` (int): The dimensionality of the resulting word embeddings. Default is 300.
- **Returns**: 
  - `list of numpy.ndarray`: A list containing the word embeddings for each word present in the input text.

#### Overview

The `Word2Vec` function is designed to generate word embeddings for a given text, which may contain words from both English and Chinese languages, using pre-trained FastText models. Initially, the function checks and downloads the FastText models for English and Chinese if they are not already downloaded. Subsequently, it loads the models and, if requested, reduces their dimensionality to the specified size.

The text is tokenized into words, and for each word, the function checks whether it contains Chinese characters. If a word contains Chinese characters, the Chinese FastText model is used to get its embedding; otherwise, the English model is used. The resulting embeddings are appended to a list which is then returned.

- **Utilizing FastText**: FastText models, which are pre-trained on a large corpus of text, are employed to generate word embeddings.
- **Support for Multiple Languages**: Specifically designed to handle texts containing both English and Chinese words by utilizing respective language models.
- **Dimensionality Reduction**: Offers the flexibility to reduce the dimensionality of the embeddings if a smaller size is desired.

#### example
```python
from HanziNLP import Word2Vec

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
result = Word2Vec(sample_sentence)
```

### 7.2 BERT Embeddings
- **Function**: `get_bert_embeddings(text, model="bert-base-chinese")`
- **Purpose**: Retrieve BERT embeddings for a specified text using a pre-trained Chinese BERT model.
- **Parameters**:
  - `text` (str): The input text for which embeddings are to be generated.
  - `model` (str): The name of the pre-trained Chinese BERT model to be utilized. Default is "bert-base-chinese."
- **Returns**: 
  - `sentence_embedding` (list): The sentence embedding represented as a list of floats.
  - `tokens` (list): The tokens associated with the sentence embedding.

#### Overview

The `get_bert_embeddings` function is engineered to extract BERT embeddings for a given text using a specified pre-trained Chinese BERT model. Initially, the function loads the designated BERT model and its corresponding tokenizer. The input text is tokenized and prepared for the model, ensuring it is truncated to a maximum length of 512 tokens to be compatible with the BERT model.

Subsequent to tokenization, the model generates predictions, and the last hidden states of the BERT model are retrieved. The sentence embedding is computed by taking the mean of the last hidden states and converting it to a list of floats. Additionally, the tokens associated with the sentence embedding are obtained by converting the input IDs back to tokens.

- **Utilizing BERT**: Leverages a pre-trained BERT model, renowned for its effectiveness in generating contextual embeddings.
- **Support for Chinese Text**: Specifically tailored to handle Chinese text by utilizing a Chinese BERT model.
- **Token Handling**: Ensures tokens are appropriately managed and returned alongside embeddings for reference and further analysis.

#### example
```python
from HanziNLP import get_bert_embeddings

embeddings, tokens = get_bert_embeddings(text, model = "bert-base-chinese") # enter the BERT Model name you wish to use from Hugging Face
print(f"Tokens: {tokens}")
print(f"Embeddings: {embeddings}")
```

## 8. Topic Modeling
HanziNLP have integrated code to easily implement LDA model to extract topics from large amount of text. More models will be updated: 

### 8.1 Latent Dirichlet Allocation (LDA) model

- **Function**: `lda_model(texts, num_topics=10, passes=15, dictionary=None)`
- **Purpose**: Train a Latent Dirichlet Allocation (LDA) model on the provided texts to extract and identify topics.
- **Parameters**:
  - `texts` (list of list of str): A list of documents, with each document represented as a list of tokens.
  - `num_topics` (int): The number of topics to extract. Default is 10.
  - `passes` (int): The number of training passes through the corpus. Default is 15.
  - `dictionary` (corpora.Dictionary, optional): An optional precomputed Gensim dictionary.
- **Returns**: 
  - `lda_model`: The trained LDA model.
  - `corpus`: The corpus used to train the model.
  - `dictionary`: The dictionary used to train the model.

#### Overview

The `lda_model` function is devised to train an LDA model on a collection of texts, facilitating the extraction and identification of underlying topics. If no precomputed dictionary is provided, the function generates a new one from the input texts. The texts are converted into a bag-of-words representation, and the LDA model is trained using specified or default parameters. The trained model, corpus, and dictionary are returned, enabling further analysis and topic visualization.

- **Topic Modeling**: Utilizes LDA, a popular topic modeling technique, to uncover latent topics in the text data.
- **Flexible Training**: Allows specification of the number of topics, training passes, and optionally, a precomputed dictionary.
- **Applicability**: Suitable for analyzing large volumes of text data to discover thematic structures.

### 8.2 LDA print topics function

- **Function**: `print_topics(lda_model, num_words=10)`
- **Purpose**: Display the top words associated with each topic from a trained LDA model.
- **Parameters**:
  - `lda_model`: The trained LDA model.
  - `num_words` (int): The number of top words to display for each topic. Default is 10.
- **Returns**: 
  - None (Outputs are printed to the console).

#### Overview

The `print_topics` function is designed to display the top words associated with each topic from a trained LDA model, providing a quick and insightful overview of the thematic essence of each topic. By iterating through each topic, it prints the topic index and the top words, aiding in the interpretability and analysis of the topics extracted by the LDA model.

- **Topic Interpretation**: Facilitates easy interpretation of the topics generated by the LDA model.
- **Customizable Output**: Allows the user to specify the number of top words to be displayed for each topic.
- **Insightful Overview**: Provides a succinct and informative overview of the primary themes present in the text data.

#### example
```python
from HanziNLP import sentence_segment, word_tokenize, lda_model, print_topics

sample_sentence = 'hello world! This is Sam.ã€‚ é™¤éä½ ä¸è¯´ã€‚æˆ‘ä»Šå¤©å°±ä¼šå¾ˆå¼€å¿ƒ,hello .youã€‚'
sentences = sentence_segment(sample_sentence)
tokenized_texts = [word_tokenize(sentence) for sentence in sentences]
lda_model, corpus, dictionary = lda_model(tokenized_texts, num_topics=5)
print_topics(lda_model)
```
#### output
``` python
Topic: 0 
Words: 0.231*"This" + 0.231*"Sam" + 0.231*"." + 0.038*"è¯´" + 0.038*"hello" + 0.038*"world" + 0.038*"!" + 0.038*"ä»Šå¤©" + 0.038*"å¼€å¿ƒ" + 0.038*"ä¼š"
Topic: 1 
Words: 0.231*"world" + 0.231*"!" + 0.231*"hello" + 0.038*"è¯´" + 0.038*"." + 0.038*"Sam" + 0.038*"This" + 0.038*"ä»Šå¤©" + 0.038*"ä¼š" + 0.038*"å¼€å¿ƒ"
Topic: 2 
Words: 0.091*"è¯´" + 0.091*"This" + 0.091*"!" + 0.091*"hello" + 0.091*"." + 0.091*"world" + 0.091*"Sam" + 0.091*"å¼€å¿ƒ" + 0.091*"ä»Šå¤©" + 0.091*"ä¼š"
Topic: 3 
Words: 0.146*"." + 0.146*"hello" + 0.146*"," + 0.146*"ä¼š" + 0.146*"å¼€å¿ƒ" + 0.146*"ä»Šå¤©" + 0.024*"è¯´" + 0.024*"Sam" + 0.024*"!" + 0.024*"world"
Topic: 4 
Words: 0.375*"è¯´" + 0.063*"hello" + 0.063*"." + 0.063*"!" + 0.063*"Sam" + 0.063*"world" + 0.063*"This" + 0.063*"ä»Šå¤©" + 0.063*"ä¼š" + 0.063*"å¼€å¿ƒ"
```

## 9. Sentiment Analysis
Sentiment Analysis is common in NLP tasks when sentiment of text could contribute to analysis in further research. While there are many ways to do sentiment analysis like using a sentiment dictionary, HanziNLP integrate the function to allow easily using pretrained BERT models or other language models on Huggin Face for text classification. 

### sentiment Function

- **Function**: `sentiment(text, model='hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2', print_all=True, show=False)`
- **Purpose**: Execute sentiment analysis on the input text utilizing the specified pre-trained model and optionally visualize the probability distribution across sentiment labels.
- **Parameters**:
  - `text` (str): The input text subject to sentiment analysis.
  - `model` (str): The identifier of the pre-trained model to be used. You can use any model on **Hugging Face** and copy the model name here to use it to classify the text. Default is 'hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2'.
  - `print_all` (bool): Indicator whether to print probabilities for all labels or only the label with the highest probability. Default is True.
  - `show` (bool): Indicator whether to display a bar chart showing the probability distribution across labels. Default is False.
- **Returns**: 
  - `dict` or `tuple`: If `print_all` is True, a dictionary containing sentiment labels and their corresponding probabilities. If `print_all` is False, a tuple containing the label with the highest probability and its corresponding probability.

#### Overview

The `sentiment` function is tailored to perform sentiment analysis on a provided text using a specified pre-trained model. Upon loading the tokenizer and model, the input text is tokenized and passed through the model to obtain output logits. These logits are then converted to probabilities using the softmax function. The labels corresponding to these probabilities are retrieved from the modelâ€™s configuration and stored in a dictionary along with their respective probabilities.

If `show` is set to True, a bar chart visualizing the probability distribution across sentiment labels is displayed. The function returns either a dictionary of all sentiment labels and their corresponding probabilities (if `print_all` is True) or a tuple containing the label with the highest probability and its corresponding probability (if `print_all` is False).

- **Sentiment Analysis**: Utilizes a specified pre-trained model to analyze the sentiment of the input text.
- **Visualization**: Optionally visualizes the probability distribution across sentiment labels using a bar chart.
- **Flexible Output**: Provides flexibility in output, allowing for detailed or concise sentiment analysis results.

#### example
```python
from HanziNLP import sentiment

text = "è¿™ä¸ªå°å…„å¼Ÿå¼¹çš„å¤ªå¥½äº†"
sentiment= sentiment(text, model = 'touch20032003/xuyuan-trial-sentiment-bert-chinese', show = True) # Enter any pretrained classification model on Hugging Face
print('sentiment =' , sentiment)
```
#### output
``` python
sentiment = {'none': 2.7154697818332352e-05, 'disgust': 2.6893396352534182e-05, 'happiness': 0.00047770512173883617, 'like': 0.9991452693939209, 'fear': 3.293586996733211e-05, 'sadness': 0.00013537798076868057, 'anger': 8.243478805525228e-05, 'surprise': 7.21854084986262e-05}
```
![Example Image](README_PIC/sentiment.png)

## Citation
The Bibliography of this package can be found [here](Bibliography.txt)

If you use **HanziNLP** in your research, please consider citing it as follows:

### APA Style

Zhan, Shi. (2023). HanziNLP (Version 0.1.0) [Software]. GitHub. [https://github.com/samzshi0529/HanziNLP](https://github.com/samzshi0529/HanziNLP)

### BibTeX Entry

For use in LaTeX documents, you can use the following BibTeX citation:

```bibtex
@misc{Zhan2023,
  author = {Zhan, Shi.},
  title = {HanziNLP},
  year = {2023},
  publisher = {GitHub},
  version = {0.1.0},
  howpublished = {\url{https://github.com/samzshi0529/HanziNLP}}
}
