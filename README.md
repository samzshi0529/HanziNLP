# HanziNLP

An **user-friendly** and **easy-to-use** Natural Language Processing package specifically designed for Chinese text analysis, modeling, and visualization.

## Table of Contents
- [Introduction](#introduction)
  - [Related Links](#related-links)
  - [Installing and Usage](#installing-and-usage)
- [Character and Word Counting](#character-and-word-counting)
- [Font Management](#font-management)
- [Word Tokenization](#word-tokenization)
  - [Stopword Management](#stopword-management)
  - [Text Segmentation](#text-segmentation)
- [Text Representation](#text-representation)
- [Text Similarity](#text-similarity)
- [Word Embeddings](#word-embeddings)
- [Topic Modeling](#topic-modeling)
- [Sentiment Analysis](#sentiment-analysis)

## Developer

To anyone using HanziNLP, big thanks to you from the developer æ–½å±•,Samuel Shi! ğŸ‰ğŸ‰ğŸ‰ 

For any improvement and more information about me, you can find via the following ways:
- **Personal Email**: samzshi@sina.com
- **Personal Webiste**: [https://www.samzshi.com/](https://www.samzshi.com/)
- **Linkedin**: [www.linkedin.com/in/zhanshisamuel](www.linkedin.com/in/zhanshisamuel)

## Introduction

Welcome to **HanziNLP** ğŸŒŸ - an ready-to-use toolkit for Natural Language Processing (NLP) on Chinese text, while also accommodating English. It is designed to be user-friendly and simplified tool even for freshmen in python. 

Moreover, HanziNLP features an interactive dashboard for dynamic insights into NLP functionalities, providing a dynamic overview and insights into various NLP functionalities.

### Related Links

- **GitHub Repository**: Explore my code and contribute on [GitHub](https://github.com/samzshi0529/HanziNLP).
- **PyPI Page**: Find me on [PyPI](https://libraries.io/pypi/HanziNLP) and explore more about how to integrate HanziNLP into your projects.

### Installation and Usage

Getting started with HanziNLP is as simple as executing a single command!

```python
pip install HanziNLP
```

## Character and Word Counting ğŸ“Š

ğŸš€ This basic function count the characters and words in your text, sparing you the manual effot of identifying and splitting Chinese words on your own. 

### char_freq and word_freq Functions
- `char_freq`: Function to calculate the frequency of each character in a given text.
- `word_freq`: Function to calculate the frequency of each word in a given text.
### Code Example
```python
from HanziNLP import char_freq, word_freq

text = "ä½ å¥½, ä¸–ç•Œ!"
char_count = char_freq(text)
word_count = word_freq(text)

print(f"Character Count: {char_count}")
print(f"Word Count: {word_count}")
```
### Output Example
```python
Charater Count: 4
Word Count: 2
```
## Font Management

When visualizing Chinese text in Python environment, font is a vital resource which is often needed from manual importing. HanziNLP have built-in list of fonts for usage right away. You can use list_fonts() to see and filter all available fonts and use get_font() to retrieve a specific font path for visualization purposes. All built-in fonts are from Google fonts that are licensed under the Open Font License, meaning one can use them in your products & projects â€“ print or digital, commercial or otherwise.

### list_fonts and get_font Functions
- `list_fonts`: List all available fonts.
- `get_font`: Retrieve a specific font for visualization purposes.

#### list_fonts() example
```python
from HanziNLP import list_fonts

# List all available fonts
list_fonts()
```
#### output
![Example Image](README_PIC/list_fonts().png)

#### get_font() example
```python
from HanziNLP import get_font

font_path = get_font('ZCOOLXiaoWei-Regular') #Enter the font_name you like in list_fonts()
```
#### output
![Example Image](README_PIC/get_font.png)

#### WordCloud Example
You can use the Chinese font_path you defined to make all kinds of plots. A wordcloud example is provided below:
```python
from PIL import Image
from wordcloud import WordCloud,ImageColorGenerator
import matplotlib.pyplot as plt

# A sample text generated by GPT-4 
text = 'åœ¨æ˜åªšçš„æ˜¥å¤©é‡Œï¼Œå°èŠ±çŒ«å’ªæ‚ é—²åœ°èººåœ¨çª—å°ä¸Šï¼Œäº«å—ç€æ¸©æš–çš„é˜³å…‰ã€‚å¥¹çš„çœ¼ç›é—ªçƒç€å¥½å¥‡çš„å…‰èŠ’ï¼Œæ—¶ä¸æ—¶åœ°è§‚å¯Ÿç€çª—å¤–å¿™ç¢Œçš„å°é¸Ÿå’Œè´è¶ã€‚å°çŒ«çš„å°¾å·´è½»è½»æ‘‡åŠ¨ï¼Œè¡¨è¾¾ç€å¥¹å†…å¿ƒçš„èˆ’é€‚å’Œæ»¡è¶³ã€‚åœ¨å¥¹çš„èº«è¾¹ï¼Œä¸€ç›†ç››å¼€çš„ç´«ç½—å…°æ•£å‘ç€æ·¡æ·¡çš„é¦™æ°”ï¼Œç»™è¿™ä¸ªå®é™çš„åˆåå¢æ·»äº†å‡ åˆ†è¯—æ„ã€‚å°èŠ±çŒ«å’ªå¶å°”ä¼šé—­ä¸Šå¥¹çš„çœ¼ç›ï¼Œæ²‰æµ¸åœ¨è¿™ç¾å¥½çš„æ—¶å…‰ä¸­ï¼Œä»¿ä½›æ•´ä¸ªä¸–ç•Œéƒ½å˜å¾—æ¸©é¦¨å’Œè°ã€‚çª—å¤–çš„æ¨±èŠ±æ ‘åœ¨å¾®é£ä¸­è½»è½»æ‘‡æ›³ï¼Œæ´’ä¸‹ä¸€ç‰‡ç‰‡ç²‰è‰²çš„èŠ±ç“£ï¼Œå¦‚æ¢¦å¦‚å¹»ã€‚åœ¨è¿™æ ·çš„ä¸€ä¸ªæ‚ æ‰˜çš„æ˜¥æ—¥é‡Œï¼Œä¸€åˆ‡éƒ½æ˜¾å¾—å¦‚æ­¤ç¾å¥½å’Œå¹³é™ã€‚'

text = " ".join(text)

# Generate the word cloud
wordcloud = WordCloud(font_path= font_path, width=800, height=800,
                      background_color='white',
                      min_font_size=10).generate(text)

# Display the word cloud
plt.figure(figsize=(5, 5), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.title("sample wordcloud")

plt.show()
```
#### output
![Example Image](README_PIC/wordcloud.png)

## Word Tokenization
Word Tokenization is a vital step in any NLP tasks. The general step is to segment the sentences, remove stopwords, and tokenize each sentences separately. The detailed instructions are introduced below. 

### Stopword Management
To remove stopwords in Chinese text, the package have built-in common stopwords lists include the following ones:
| Header 1 | Header 2 | Header 3 |
|----------|----------|----------|
| Row 1, Col 1 | Row 1, Col 2 | Row 1, Col 3 |
| Row 2, Col 1 | Row 2, Col 2 | Row 2, Col 3 |
| Row 3, Col 1 | Row 3, Col 2 | Row 3, Col 3 |

#### list_stopwords and load_stopwords Functions
- `list_stopwords`: List all available stopwords.
- `load_stopwords`: Load stopwords from a specified file.

##### list_stopwords example
```python
from HanziNLP import list_stopwords

list_stopwords()
```
##### output 
![Example Image](README_PIC/list_stopwords.png)

##### load_stopwords example
```python
from HanziNLP import load_stopwords

stopwords = load_stopwords('common_stopwords.txt') # Enter the txt file name here
```

### Tokenization Specifics

#### sentence_segment and word_tokenize Functions
- `sentence_segment`: Segment the input text into sentences.
- `word_tokenize`: Tokenize the input text into words and remove stopwords.

## Text Representation

### BoW, ngrams, TF_IDF, and TT_matrix Functions
- `BoW`: Generate a Bag of Words representation of the input text.
- `ngrams`: Generate n-grams from the input text.
- `TF_IDF`: Generate a TF-IDF representation of the input text.
- `TT_matrix`: Generate a term-term matrix of the input text.

## Text Similarity

### text_similarity Function
- `text_similarity`: Calculate the similarity between two texts using various methods.

## Word Embeddings

### Word2Vec and get_bert_embeddings Functions
- `Word2Vec`: Obtain word embeddings using the FastText model.
- `get_bert_embeddings`: Obtain word embeddings using the BERT model.

## Topic Modeling

### lda_model and print_topics Functions
- `lda_model`: Train an LDA model on the input text.
- `print_topics`: Print the topics identified by the LDA model.

## Sentiment Analysis

### sentiment Function
- `sentiment`: Perform sentiment analysis on the input text using a specified pre-trained model.

---

Feel free to modify the descriptions and add any additional information or usage examples that you think would be helpful for users of your package!
